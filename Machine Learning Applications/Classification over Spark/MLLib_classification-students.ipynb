{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification tools in MLLib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn to manage the different classification tools avaliable in MLLIb. Furthermore, we will extend some of these tools to a multiclass classification scenario.\n",
    "\n",
    "Along this lab session, we will use the MNIST dataset, which is a widely used dataset in machine learning for testing classification algorithms. The dataset has 60.000 training patterns and additional 10.000 observations for testing purposes and each data corresponds to a digit image with 780 pixels. The goal of the problem is automatically classify a new image among the ten possible digits.\n",
    "\n",
    "The outline of this notebook is:\n",
    "\n",
    "**1. Data reading and preprocessing**\n",
    "    1. Read data.\n",
    "    2. Data analysis.\n",
    "    3. Data normalization.\n",
    "    4. Split data for training, validation, and testing.\n",
    "**2. Model training. Here, we will start analyzing multiclass classification approaches:**\n",
    "    1. Decission trees\n",
    "    2. Random Forest\n",
    "    \n",
    "**3. Evaluating the model performance over a test dataset**\n",
    "\n",
    "**4. Selecting the model parameters by cross-validation**\n",
    "\n",
    "**5. Creating a multiclass classifier from a binary Support Vector Machine**\n",
    "\n",
    "**6. Interpretability analysis: feature selection approaches**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data reading and preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read data**\n",
    "Start downloading the MNIST dataset from:\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist\n",
    "\n",
    "After completing this notebook, you can analyze the scalability of the different approaches over a larger dataset by using the large version of the MNIST dataset (with 8.100.000 patterns):\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#mnist8m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard method to load text files is:\n",
    "sc.textFile(\"file_name\")\n",
    "which automatically creates an RDD with as many elements as lines in the data file.\n",
    "Run the following cell and analyze:\n",
    "1. The number of lines in the file \n",
    "2. The content of a line\n",
    "\n",
    "How can we process this file to transform each line into data which are easy to handle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# You need to include mnist file in your working directory\n",
    "lines = sc.textFile(\"mnist\")\n",
    "# Examine dataset format\n",
    "# 1. Number of lines\n",
    "n_lines = #FILL \n",
    "print 'Number of lines: ' + str(n_lines)\n",
    "# 2. Content of the first line\n",
    "line = # FILL \n",
    "print 'A line content:'\n",
    "print line\n",
    "# 3. Data type of a line\n",
    "type_data = # FILL \n",
    "print 'Data type:'\n",
    "print type_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(n_lines, 60000, 'incorrect result: number of file lines is uncorrect')\n",
    "Test.assertEquals(line[:10], '5 153:3 15', 'incorrect result: first line is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib includes different data types (see http://spark.apache.org/docs/latest/mllib-data-types.html), such as, Local vectors, Labeled points, Local matrices or Distributed matrices. In supervised learning algorthms, the default data type is the  “labeled point” (see http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point).\n",
    "\n",
    "So we need to transform our data file to an RDD of LabeledPoint. Both features and label fields in a LabeledPoint are of type Double; however, the input dataset has both the features and label in string format. So, as you can imagine, the proccess to transform input data to numeric values can be a bit thedious...\n",
    "\n",
    "However, this data file has an especific format, known as LIBSVM text file format, where each line is:\n",
    "\n",
    "    label index1:value1 index2:value2 ...\n",
    "\n",
    "That is, it is representing a labeled sparse feature vector.\n",
    "\n",
    "Furthermore, MLLib includes a specific funtion:\n",
    "\n",
    "    loadLibSVMFile(sc, path, numFeatures=-1, minPartitions=None, multiclass=None) \n",
    "\n",
    "which directly reads this format data file and returns an RDD of LabeledPoint elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the MNIST data as LabelPoint elements.\n",
    "\n",
    "Note: we have explicitely defined the number of features to be sure that the sparse feature vectors are created with all the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, \"mnist\", numFeatures= 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to examine the content of the RDD data: \n",
    "    1. Get the first pattern\n",
    "    2. Extract its label \n",
    "    3. Extract its features\n",
    "Note: take into account that the LabelePoint type includes its own methods to extract the label and the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# 1. Get the first pattern\n",
    "dat = data.first()\n",
    "# 2. Extract its label \n",
    "label = # <FILL IN>\n",
    "print label\n",
    "# 3. Extract its features\n",
    "features = # <FILL IN>\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(label, 5, 'incorrect result: label is uncorrect')\n",
    "Test.assertEquals(sum(features), 27525, 'features are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data analysis**\n",
    "\n",
    "To analyze the data, let's start plotting each digit image. For this purpose, you can use the code provided in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def plot_data(images, h, w, n_row=1, n_col=10):\n",
    "    \"\"\"Plots the set of images provided in images\n",
    "\n",
    "    Args:\n",
    "        images (list of sparse vectors or numpy arrays): list of images where each image contains the \n",
    "            features corresponding to the pixels of an image.  \n",
    "        h: heigth of the image (in number of pixels).\n",
    "        w: width of the image (in number of pixels).\n",
    "        n_row: Number of rows to use when plotting all the images\n",
    "        n_col: Number of columns to use when plotting all the images\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)      \n",
    "        try:\n",
    "            img = images[i].toArray()\n",
    "        except:\n",
    "            img = images[i]\n",
    "            \n",
    "        plt.imshow(img.reshape((h, w)), cmap=plt.cm.jet)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Define the height and width of the images\n",
    "h= 28\n",
    "w =28\n",
    "\n",
    "# From the data RDD, create a new RDD where each element only has the features (pixel values) of each image\n",
    "features = #FILL\n",
    "\n",
    "# Pick up 10 images and plot them with plot_data() function\n",
    "images= #FILL\n",
    "plot_data(images, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute some statistics of the data set.\n",
    "\n",
    "For this purpose, we can use the Statistics MLLIB library which let us compute (distribuitelly) some statistical parameters of the features, such as, the mean, the standard deviation, the maximum and minimun values, or the number of times that a pixel is not zero.\n",
    "\n",
    "Complete the following cell to compute all the abovementioned statistics.\n",
    "\n",
    "Note: Use the Statistics.colStats( ) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Compute summary statistics using the rdd of features as input.\n",
    "stats = # FILL\n",
    "\n",
    "# Extract the desired statistics\n",
    "mean = # FILL\n",
    "variance = #FILL\n",
    "maximum = # FILL\n",
    "minimum = # FILL\n",
    "numNonzeros = # FILL\n",
    "\n",
    "# Use the plot_data function to plot them\n",
    "statistics = [mean, variance, maximum, minimum, numNonzeros]\n",
    "plot_data(statistics, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(np.sum(mean.ravel()),0), 26122, 'incorrect result: mean is uncorrect')\n",
    "Test.assertEquals(np.round(np.sum(variance.ravel()),0), 3428503, 'incorrect result: variance is uncorrect')\n",
    "Test.assertEquals(np.round(np.sum(maximum.ravel()),0), 172093, 'incorrect result: maximum is uncorrect')\n",
    "Test.assertEquals(np.round(np.sum(minimum.ravel()),0), 0, 'incorrect result: minimum is uncorrect')\n",
    "Test.assertEquals(np.round(np.sum(numNonzeros.ravel()),0), 8994156, 'incorrect result: numNonzeros is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data normalization**\n",
    "\n",
    "Now, let's normalize the data. Usually, the data normalization consist of two steps:\n",
    "1. Remove the mean of each feature\n",
    "2. Reescale each feature to make them have unitary standard deviation\n",
    "\n",
    "Due to we are working with sparse data (most of the input fatures are zero), if we removed the mean, we would make zero values to take a a non-null value, which will increase the size (in memory) of the data set. To avoid this, here we are only going to reescale the data.\n",
    "\n",
    "Complete the following cell to reescale the training data by making use of the StandardScaler method of MLLIB (http://spark.apache.org/docs/latest/mllib-feature-extraction.html#standardscaler).\n",
    "\n",
    "Note 1: until now, we have only loaded the training data, so use all the pattern in data variable to fit the Scaler method.\n",
    "\n",
    "Note 2: StandardScaler method has two input variables, 'withMean' and 'withStd', which let you select, respectivelly, whether the mean and standard deviation are corrected or not. By default, 'withMean' is set to False and 'withStd' to True, that is, only standard deviation is corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create two new RDD by extracting the labels and features of the data\n",
    "label = # FILL IN\n",
    "features = # FILL IN\n",
    "\n",
    "# Define the StandardScaler() object and fit it with the data features \n",
    "scaler = # FILL IN\n",
    "\n",
    "# Normalize the data features\n",
    "features_norm = # FILL IN\n",
    "\n",
    "# Create a new RDD of LabeledPoint data using the normalized features\n",
    "# 1. Construct a RDD of tuples (label, featatures): check zip() method of RDD objects\n",
    "data_norm = # FILL IN\n",
    "# 2. Create the label point RDD\n",
    "data_LP =  # FILL IN\n",
    "\n",
    "print data_LP.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(sum(features_norm.first()),0), 319, 'incorrect result: normalized featues are uncorrect')\n",
    "Test.assertEquals(data_LP.first().label, 5, 'incorrect result: normalized Label Point data are uncorrect')\n",
    "Test.assertEquals(np.round(sum(data_LP.first().features),0), 319, 'incorrect result: normalized Label Point data are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create training, validation and test partitions**\n",
    "\n",
    "In this subsection, let’s split the normalized dataset into training and validation data. We will use 40% of the data for training a model and 60% for validating the hyperparameters of the different learning algorithms.\n",
    "To save computational time, cache both normalized training and validation RDDs, sicen we will use them several times.\n",
    "\n",
    "You can use the randomSplit() method for this purpose.\n",
    "\n",
    "Note: when you call to randomSplit, please, set seed=0 for comparison purposes: \n",
    "    randomSplit([....], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "# Create trainign and validation partitions\n",
    "(trainingData, valData) = # FILL\n",
    "\n",
    "# Our learning algorithms will make several passes over these datasets, so let’s cache these RDD in memory\n",
    "trainingData.cache()\n",
    "valData.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(sum(trainingData.first().features),0), 355, 'incorrect result: training data are uncorrect')\n",
    "Test.assertEquals(np.round(sum(valData.first().features),0), 319, 'incorrect result: validation data are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's load the test data from the \"mnist.t\" file. To be able to use it for testing purposes, we will also have to normalize them using the normalization parameters learned with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Load test data\n",
    "data = MLUtils.loadLibSVMFile(sc, \"mnist.t\", numFeatures=784)\n",
    "\n",
    "# Normalize test data:\n",
    "\n",
    "# 1. Create two new RDD by extracting the labels and features of the data\n",
    "label = # FILL IN\n",
    "features = # FILL IN\n",
    "\n",
    "# 2. Normalize the data features (use the scaler object fitted with the training data)\n",
    "features_norm = # FILL IN\n",
    "\n",
    "# 3. Create a new RDD of LabeledPoint data using the normalized features and cache it!\n",
    "# 3.1 RDD with tuples (label, features)\n",
    "test = # FILL IN\n",
    "# 3.2 RDD with Label Point data\n",
    "testData = # FILL IN\n",
    "\n",
    "testData.cache()\n",
    "testData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(sum(testData.first().features),0), 199, 'incorrect result: training data are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Model training**\n",
    "\n",
    "MLLIb includes several classification methods. Most of them are only implemented for solving binary problems. As we intend to solve a multiclass problem, let's start using the classifiers with multiclass implementations:\n",
    "* Decision trees \n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision trees**\n",
    "\n",
    "As we already know, a decision tree works by selecting the most discriminative features and setting different threholds over them, in such a way, that each tree node splits the training data in different subsets with the aim to find the node purity (the data partitions belongs to a single class). For this purpose, a purity measure, such as the gini index, is used to select both the most discriminative features and the threshold to apply.\n",
    "\n",
    "Review http://spark.apache.org/docs/latest/mllib-decision-tree.html for implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the necessary code to train a DecisionTree and creates in the variable model a DecisionTreeModel. Note that all free parameters (such as the maximum  depth of the tree) have been set by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "#  Train a DecisionTree model\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model_tree = DecisionTree.trainClassifier(trainingData, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forests**\n",
    "\n",
    "A Random Forest builds an ensemble of trees by training multiple trees in parallel (with different data and features) and combining their outputs. In the case of classification, the combination is carried out by a majority vote.\n",
    "\n",
    "See https://spark.apache.org/docs/1.2.0/mllib-ensembles.html#random-forests for further details.\n",
    "\n",
    "Next cells includes the code to train a Random Forest for some default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "#  Train a RandomForest model\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model_RF = RandomForest.trainClassifier(trainingData, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     numTrees=50, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Evaluating the model performance over a test dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to evaluate the performance of the different models that we have trained, let's create a function that given a MLLib classification model and an RDD with a data set of LabelPoints, compute the classification error of the model over the given data.\n",
    "This function has to follow these steps:\n",
    "- Compute the model output over the data using the model.predict() method (MLLib classification models include this method). Note that this method only receives as input the data features (instead of complete LabelPoint)\n",
    "- Create an RDD with the same length of the data with tuples given by the original label of a data and its corresponding output.\n",
    "- Compute the test error: number of missclassified data divided by the number of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "def compute_classifier_error(model, Data):\n",
    "\n",
    "    \"\"\" Compute the classification error of the model over the samples given in Data.\n",
    "\n",
    "    Args:\n",
    "       model: MLLib classification model\n",
    "       Data: an RDD with a data set of LabelPoints\n",
    "    \n",
    "    Returns:\n",
    "        Int: A single value, between 0 and 1, indicating the classification error. \n",
    "        A value of 1 indicates that all the samples are missclassified (100% of error)\n",
    "        and a value of 0 that all the samples are correctly classified (100% accuracy).\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    # 1. Compute the model output\n",
    "    predictions = # FILL IN\n",
    "    # 2. Create an RDD of tuples (label, output)\n",
    "    labelsAndPredictions = # FILL IN\n",
    "    # 3. Compute test error\n",
    "    testErr = # FILL IN\n",
    "\n",
    "    return testErr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to compute the test error of the tree classifier and the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "# Test error of the decision tree\n",
    "tree_testErr = # FILL IN\n",
    "print('Tree test error = ' + str(tree_testErr))\n",
    "\n",
    "# Test error of the random forest \n",
    "RF_testErr = # FILL IN\n",
    "print('Random Forest test error = ' + str(RF_testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*tree_testErr,0), 31, 'incorrect result: decision tree error is uncorrect')\n",
    "Test.assertEquals(np.round(100*RF_testErr,0), 22, 'incorrect result: RF error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Selecting the model parameters by cross-validation**\n",
    "\n",
    "Until now, both tree and RF model have used predefined parameter values. Here, we are going to adjust the free parameters of these models by cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision trees: adjusting the tree depth**\n",
    "\n",
    "The most critical parameter of a decision tree is its maximum depth. Note that deeper trees are more expressive (potentially allowing higher accuracy), but they are also more costly to train and are more likely to overfit.\n",
    "\n",
    "The following cell explores different tree depths and evaluate its over the validation data to, finally, select the optimum tree depth as the depth value which provides the minimum classification error over the validation data. Complete the missing code lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables \n",
    "bestModel = # FILL IN\n",
    "best_error = # FILL IN\n",
    "best_depth = # FILL IN\n",
    "# Range of depth values to explore\n",
    "depth_params = [5, 10, 15]\n",
    "\n",
    "for depth_value in depth_params:\n",
    "    # Train a decision tree fixing maxDepth to depth_value \n",
    "    model_tree = DecisionTree.trainClassifier(trainingData, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=#FILL IN, maxBins=32)\n",
    "    \n",
    "    # Compute the model error over the validation data (use compute_classifier_error function)\n",
    "    tree_valErr = # FILL IN\n",
    "    \n",
    "    print 'Tree depth is ' + str(depth_value) + ' and the validation error is '+ str(tree_valErr) \n",
    "    \n",
    "    # If the error has reduced, save the model, the optimum depth and error in bestModel, best_depth and best_error variables\n",
    "    if (tree_valErr < best_error):\n",
    "            bestModel = # FILL IN\n",
    "            best_depth = # FILL IN\n",
    "            best_error = # FILL IN\n",
    "            \n",
    "print 'Optimum tree depth: ' + str(best_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the validation error behaviour with the tree depth. Is this the expected behaviour? \n",
    "\n",
    "Taking into account the trade-off computational cost vs. accuracy, which tree depth will you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(best_depth, 15, 'incorrect result: best_depth is uncorrect')\n",
    "Test.assertEquals(np.round(100*best_error,0), 15, 'incorrect result: best_error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluate the error the selected model (bestModel) over the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Finally, evaluate the test error over the best model\n",
    "tree_finalErr = # FILL IN\n",
    "\n",
    "print 'Final test error of the validated tree: ' + str(tree_finalErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*tree_finalErr,0), 15, 'incorrect result: decision tree test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest: adjusting the number of trees**\n",
    "\n",
    "In these case, the most critical parameter is the number of trees in the forest ('numTrees'). Note that increasing the number of trees will decrease the variance in predictions, improving the generalization capability of the ensemble. \n",
    "\n",
    "Complete the following cell to adjust this parameter by cross validation, that is, selecting the value which provides the minimum classification error over the validation data. \n",
    "\n",
    "Note: it is also quite common adjusting the tree depth. However, in this exercise, due to computational reasons, we are going to prefix its value to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "import numpy as np\n",
    "\n",
    "# Initialize variables \n",
    "bestModel = # FILL IN\n",
    "best_error =  # FILL IN\n",
    "best_depth = # FILL IN\n",
    "best_ntrees =  # FILL IN\n",
    "\n",
    "# Range of values to explore\n",
    "ntrees_params = [20, 50, 100] \n",
    "\n",
    "for ntrees_value in ntrees_params:\n",
    "    # Train a RandomForest model.\n",
    "    model_RF = RandomForest.trainClassifier(trainingData, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     numTrees= # FILL IN, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "    # Compute the model error over the validation data (use compute_classifier_error function)\n",
    "    RF_valErr =  # FILL IN\n",
    "\n",
    "    print 'Number of trees is ' + str(ntrees_value) + ' and the validation error is '+ str(RF_valErr) \n",
    "    # Check if the error has improved. If it has, save the model, the optimum depth and error in bestModel, best_depth and best_error variables\n",
    "    if (RF_valErr < best_error):\n",
    "            bestModel =  # FILL IN\n",
    "            best_ntrees  =  # FILL IN\n",
    "            best_error =  # FILL IN\n",
    "\n",
    "print 'Optimum number of trees: ' + str(best_ntrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the validation error behaviour with the number of trees. Is this the expected behaviour? \n",
    "\n",
    "Taking into account the trade-off computational cost vs. accuracy, which number of trees will you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(best_ntrees, 100, 'incorrect result: best_ntrees is uncorrect')\n",
    "Test.assertEquals(np.round(100*best_error,0), 15, 'incorrect result: best_error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluate the error the selected model (bestModel) over the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Finally, evaluate the test error over the best model\n",
    "tree_finalErr =  # FILL IN\n",
    "\n",
    "print 'Final test error of the validated tree: ' + str(tree_finalErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*tree_finalErr,0), 14, 'incorrect result: best_error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Creating a multiclass classifier from a binary Support Vector Machine**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLLib includes a distributed SVM implementation, but it is only avaliable for binary problems (see MLLIb help at: http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms)\n",
    "\n",
    "As we are working with a multiclass problem, let's adapt this implementation to be used in a  1vs. all fashion and aplly it to our multiclass problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solving  a single 1 vs. all problem**\n",
    "\n",
    "Let's start considering a single 1 vs. all problem, for instance, let's consider that we want to classify the digit '0' from the remaining digits.\n",
    "\n",
    "Then, we will procced as follows:\n",
    "1. Convert the training labels to the 1 vs. all scheme\n",
    "2. Train a binary SVM model with the new labels\n",
    "3. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Convert the training labels to the 1 vs. all scheme**\n",
    "\n",
    "Create a convert_label() funtion to generate the labels associated to the 1 vs. all problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "def convert_label(label, label1):\n",
    "    \"\"\"Produce a 1 vs. all label encoding for a single label and the label to be included in the class 1.\n",
    "\n",
    "    Args:\n",
    "        label (int, str): the label to be coded \n",
    "        label1 (int, str): the label to be included in the class 1.\n",
    "    \n",
    "    Returns:\n",
    "        Int: A single value indicating the label (0 or 1) of the 1 vs. all problem.\n",
    "    \"\"\"\n",
    "    # <FILL IN> : use all code lines that you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select as the class 1 the digit '0' and transform the labels of both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "label_1 = 0\n",
    "# Transform the labels of training data\n",
    "trainingData_1vsall= # FILL IN\n",
    "# Transform the labels of test data\n",
    "testData_1vsall= # FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(sum(trainingData_1vsall.first().features),0), 355, 'incorrect result: trainingData_1vsall are uncorrect')\n",
    "Test.assertEquals(np.round(sum(testData_1vsall.first().features),0), 199, 'incorrect result: testData_1vsall are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Train a binary SVM**\n",
    "\n",
    "Note: The  SVMWithSGD.train( ) has several parameters related to the SGD search and other ones associated to the SVM. The next cell only includes the latter ones in the training call, since we will have to adjust some of them along this notebook. The reaming parameters are set by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "model = SVMWithSGD.train(trainingData_1vsall, regParam=0.01, regType='l2', intercept=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Compute the test error of this model**\n",
    "\n",
    "Here, you can use the function compute_classifier_error( ) of the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "error_1vsall = # FILL IN\n",
    "\n",
    "print(\"Test Error os 1 vs. all model (to classify class 0) is: \" + str(error_1vsall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*error_1vsall,0), 2, 'incorrect result: best_error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing a 1 vs. all multiclass SVM**\n",
    "\n",
    "Starting from the previous function, let's create a 1 vs. all multiclass SVM. For these purpose, we need to implement:\n",
    "1. A function to train the set of 1 vs. all SVM \n",
    "2. A function to compute the output of the 1 vs. all set of classifiers.\n",
    "3. A function to compute the error over a set of data\n",
    "\n",
    "\n",
    "Note: The function includes the model.clearThreshold() command to transform the model outputs from discrete values (labels) to real values (smooth or continuous outputs). This will be necessary to later combine the output of different SVM in a 1 vs. all fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Let's create a training function to build all 1 vs. all SVMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "def train_1vsall_SVM(trainingData, regParam=0.01, regType='l2'):\n",
    "    \"\"\"Produce a list of SVM models solving all the 1 vs. all problems\n",
    "\n",
    "    Args:\n",
    "        trainingData (RDD of labeled points): the training data to adjust the model\n",
    "        regParam: The regularizer parameter (default: 0.01).\n",
    "        regType: The type of regularizer used for training our model (default: “l2”). Allowed values:\n",
    "                “l1” for using L1 regularization\n",
    "                “l2” for using L2 regularization\n",
    "                None for no regularization\n",
    "        \n",
    "    Returns:\n",
    "        List of SVM models: A list of length number of classes where each element is a tuple (lab, model). Variable\n",
    "            lab indicates the label 1 of the 1 vs. all problem and model is the SVM model solving the problem\n",
    "    \"\"\"\n",
    "    # Get all possible labels from training data\n",
    "    labels = # FILL IN\n",
    "    # Initialize the list of models to be returned\n",
    "    list_models = []\n",
    "    for lab in labels:\n",
    "        # Convert labels of trainingData to 1 vs. all format (use convert_label function)\n",
    "        trainingData_1vsall=# FILL IN\n",
    "        # Train the SVM model with 1 vs. all data\n",
    "        model = # FILL IN\n",
    "\n",
    "        # Modify the model to get smooth outputs\n",
    "        model.clearThreshold()\n",
    "        # Create the tuple (lab, model) and add it to the list of models\n",
    "        # FILL IN\n",
    "        \n",
    "    return list_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Let's create a function to compute the output of a single data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "import numpy as np\n",
    "def output_1vsall_SVM(data, list_models):\n",
    "    \"\"\"Compute the output of a list of 1 vs. all SVM models for a test data \n",
    "\n",
    "    Args:\n",
    "        data (labeled point): data to be evaluated over the 1 vs. all SVM model\n",
    "        list_models: a list of length number of classes where each element is a tuple (lab, model). Variable \n",
    "            lab indicates the label 1 of the 1 vs. all problem and model is the SVM model solving the problem\n",
    "    Returns:\n",
    "        Output_label: The label estimated by the 1 vs. all model for the data\n",
    "    \"\"\"\n",
    "    # Split the tuples of list_models into a list of labels and a list of models (you can use zip() method)\n",
    "    labels, models = # FILL IN\n",
    "    \n",
    "    outputs = []\n",
    "    # For each model...\n",
    "    for model in models:\n",
    "        # Compute the output over the test data\n",
    "        out = # FILL IN\n",
    "        # Add this output to outputs list\n",
    "        # FILL IN\n",
    "        \n",
    "    # Get the test output label as the label associated to the model with the maximum output value\n",
    "    pos = # FILL IN\n",
    "    Output_label = # FILL IN\n",
    "    return Output_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Let's create a function to compute the error over a set of data**\n",
    "\n",
    "Note that the SVM final output is a real value, so you can compute the number of errors as the number of products label$\\times$output lower than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "def compute_1vsall_SVM_error(list_models, Data):\n",
    "\n",
    "    \"\"\" Compute the classification error of the 1 vs. all SVM model over the samples given in Data.\n",
    "\n",
    "    Args:\n",
    "       list_models: a list of length number of classes where each element is a tuple (lab, model). Variable \n",
    "            lab indicates the label 1 of the 1 vs. all problem and model is the SVM model solving the problem\n",
    "       Data: an RDD with a data set of LabelPoints\n",
    "    \n",
    "    Returns:\n",
    "        Int: A single value, between 0 and 1, indicating the classification error. \n",
    "        A value of 1 indicates that all the samples are missclassfied and a value \n",
    "        of 0 that all the samples are correctly classified.\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    # 1. Compute the model output \n",
    "    predictions = # FILL IN\n",
    "    # 2. Create an RDD of tuples (label, output)\n",
    "    labelsAndPredictions = # FILL IN\n",
    "    # 3. Compute test error \n",
    "    testErr = # FILL IN\n",
    "\n",
    "    return testErr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the above functions to train the multiclass model and evaluate it over all test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Train the 1 vs. all SVM models (use multiclass_SVM() function with default parameters)\n",
    "multiclass_SVM = # FILL IN\n",
    "# Compute the test error\n",
    "error_1vsall = # FILL IN\n",
    "print(\"Test Error of the 1 vs. all SVM = \" + str(error_1vsall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*error_1vsall,0), 12, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Cross validating the regularization parameter**\n",
    "\n",
    "To really obtain the SVM performance, we should cross validate the regularization parameter (C value), since its value is critical to obtain a good performance. Complete the following cell to adjust this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Initialize variables \n",
    "bestModel = # FILL IN\n",
    "best_error = # FILL IN\n",
    "best_C = # FILL IN\n",
    "# Range of depth values to explore\n",
    "C_params = [0.01, 0.1, 1]\n",
    "\n",
    "for C_value in C_params:\n",
    "    # Train the 1 vs. all SVM models (set regularization parameter to C_value)\n",
    "    multiclass_SVM = # FILL IN\n",
    "    \n",
    "    # Compute the model error over the validation data \n",
    "    error_1vsall = # FILL IN\n",
    "    \n",
    "    print 'C value is ' + str(C_value) + ' and the validation error is '+ str(error_1vsall) \n",
    "    \n",
    "    # If the error has reduced, save the model, the optimum depth and error in bestModel, best_depth and best_error variables\n",
    "    if (error_1vsall < best_error):\n",
    "            bestModel = # FILL IN\n",
    "            best_C = # FILL IN\n",
    "            best_error = # FILL IN\n",
    "            \n",
    "print 'Optimum C value is: ' + str(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(best_C, 0.1, 'incorrect result: best_C is uncorrect')\n",
    "Test.assertEquals(np.round(100*best_error,0), 12, 'incorrect result: best_error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Interpretability analysis: feature selection approaches**\n",
    "\n",
    "The MNIST dataset has a lot of features (pixels) which are useless for the classification, for instance, some of the are constant overall the data, so they haven't any discriminatory capability.\n",
    "\n",
    "In this last section, let's implement some easy (but efficient) distribuited feature selection approaches, so that we can extract the most relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Remove features with zero variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start removing all pixels which are zero over all the images (background pixels), that is, its variance is zero. For this purpose, follow these steps:\n",
    "1. Comupute the number of non zeros in each pixel\n",
    "2. Select variables to keep\n",
    "3. Create a function which removes the no desired features from the data\n",
    "4. Evaluate the classification performance after removing useless pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "** 1. Compute the number of zeros in each pixel**\n",
    "\n",
    "Use Statistics.colStats() funtion to compute the number of non zeros in each pixel (review first section of this notebook). \n",
    "\n",
    "Note: we will compute this over the training data  and, later, apply the selection over trainign, validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "features = trainingData.map(lambda x: x.features)\n",
    "\n",
    "# Compute column summary statistics.\n",
    "stats = # FILL IN\n",
    "plot_data([stats.numNonzeros()], h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Select variables to keep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "idx_keep = # FILL IN\n",
    "print('Number of selected features = ' + str(len(idx_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(sum(idx_keep), 281576, 'incorrect result: idx_keep is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Removes the no desired features from the data**\n",
    "\n",
    "Use the following function to remove the useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def remove_features(all_features, idx_keep):\n",
    "    \"\"\" From all_features vector it selects the features given in  idx_keep and it returns them by means of a \n",
    "    Sparse Vector\n",
    "\n",
    "    Args:\n",
    "       all_features: SparseVector with the feature values\n",
    "       idx_keep: indexes with the positions to keep\n",
    "    \n",
    "    Returns:\n",
    "        SparseVector with the selected features\n",
    "    \"\"\"\n",
    "    values = all_features.toArray()[idx_keep]\n",
    "    val_nonzero = np.where(values>0)[0]\n",
    "        \n",
    "    return SparseVector(len(idx_keep), val_nonzero, values[val_nonzero])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Remove features from training data\n",
    "trainingData_sel = # FILL IN\n",
    "# Remove features from validation data\n",
    "valData_sel = # FILL IN\n",
    "# Remove features from test data\n",
    "testData_sel = # FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(sum(trainingData_sel.first().features),0), 355, 'incorrect result: trainingData_sel is uncorrect')\n",
    "Test.assertEquals(np.round(sum(valData_sel.first().features),0), 319, 'incorrect result: valData_sel is uncorrect')\n",
    "Test.assertEquals(np.round(sum(testData_sel.first().features),0), 199, 'incorrect result: testData_sel is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluate the classification performance after removing useless pixels**\n",
    "\n",
    "Here, let's use a tree with default parameters. For comparison purposes, remember that the test error using all the features is around 30%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "#  Train a DecisionTree model with selected features\n",
    "model_tree = DecisionTree.trainClassifier(trainingData_sel, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Test error of the decision tree\n",
    "tree_testErr = compute_classifier_error(model_tree, testData_sel)\n",
    "print('Tree test error = ' + str(tree_testErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*tree_testErr,0), 31, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Remove features with a low variance**\n",
    "\n",
    "Now, let's modify the code of the previous section to remove the features which variance is lower than a given threhold. \n",
    "\n",
    "Complete the next cell following the indications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "# Set a variance threshold\n",
    "th_var = 0.5\n",
    "\n",
    "# 1. Compute the variance with Statistics.colStats( )\n",
    "variance = # FILL IN (you may need several code lines to compute this)\n",
    "\n",
    "# 2. Get the positions of the features to keep\n",
    "idx_keep = # FILL IN \n",
    "\n",
    "# 3. Remove features from training, validation and test data \n",
    "trainingData_sel = # FILL IN \n",
    "valData_sel = # FILL IN \n",
    "testData_sel = # FILL IN \n",
    "\n",
    "# 4. Evaluate performance with a decision tree\n",
    "# Train the model\n",
    "model_tree = DecisionTree.trainClassifier(trainingData_sel, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Compute its test error \n",
    "tree_testErr = # FILL IN \n",
    "print('Tree test error = ' + str(tree_testErr))\n",
    "print('Number of selected features = ' + str(len(idx_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*tree_testErr,0), 31, 'incorrect result: test error is uncorrect')\n",
    "Test.assertEquals(len(idx_keep), 686, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can imagine, the final performance depends on the threshold over the variance, so we should select this value by cross validation. Please, complete the following code cell to select the optimum value of the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Initialize variables \n",
    "best_val_error =# FILL IN \n",
    "final_test_error = # FILL IN \n",
    "best_th = # FILL IN \n",
    "best_num_var_sel = # FILL IN \n",
    "\n",
    "# Range of threshold values to explore\n",
    "th_range = [0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# Compute the variance \n",
    "variance = # FILL IN (you may need several code lines to compute this)\n",
    "\n",
    "for th_var in th_range:\n",
    "    \n",
    "    # 2. Get the positions of the features to keep\n",
    "    idx_keep = # FILL IN\n",
    "    # Compute the number of selected features\n",
    "    num_var_sel = # FILL IN\n",
    "\n",
    "    # 3. Remove features from training, validation and test data \n",
    "    trainingData_sel = # FILL IN\n",
    "    valData_sel = # FILL IN\n",
    "    testData_sel = # FILL IN\n",
    "    \n",
    "    # 4. Evaluate performance with a decision tree\n",
    "    # Train the model\n",
    "    model_tree = DecisionTree.trainClassifier(trainingData_sel, numClasses=10, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "    # Compute its validation error \n",
    "    tree_valErr = # FILL IN\n",
    "    # Compute its test error\n",
    "    tree_testErr = # FILL IN\n",
    "    \n",
    "    print 'Threshold value is ' + str(th_var) + ', the number of selected features is ' + str(num_var_sel) + ' and the validation error is '+ str(tree_valErr) \n",
    "    \n",
    "    # If the error has reduced, save the model, ....\n",
    "    if (tree_valErr < best_error):\n",
    "            best_th = # FILL IN\n",
    "            best_val_error = # FILL IN\n",
    "            best_num_var_sel = # FILL IN\n",
    "            final_test_error = # FILL IN \n",
    "            \n",
    "print 'Optimum threshold value is: ' + str(best_th)\n",
    "print 'The test error is: ' + str(final_test_error)\n",
    "print 'The number of selected features is: ' + str(best_num_var_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(100*final_test_error,0), 31, 'incorrect result: test error is uncorrect')\n",
    "Test.assertEquals(best_th, 0.5, 'incorrect result: best_th is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Remove features with L1 regularization: L1-SVM**\n",
    "\n",
    "As we know, L1 regularization provides sparsity over the vector weights. If we have a linear model, this can provide a feature selection.\n",
    "\n",
    "So, in this section, let's use the above multiclass SVM implemention using an L1 regularization. In this way, we will obtain both a classifier and a feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Initialize variables \n",
    "C_value = 0.1\n",
    "\n",
    "# Train the 1 vs. all SVM models (set regularization parameter to C_value)\n",
    "# Note that we have added the parameter regType='l1'\n",
    "multiclass_SVM = train_1vsall_SVM(trainingData, regParam=C_value, regType='l1')\n",
    "    \n",
    "# Compute the model error over the test data \n",
    "error_1vsall = # FILL IN\n",
    "\n",
    "# Analyze the number of zero weigths: compute the positions where all the vectors are zero \n",
    "# multiclass_SVM is a list of tuples (class, model), getting the parameter \n",
    "# model.weights you can access to the vector weights of each SVM\n",
    "\n",
    "pos_sel = # FILL IN (You may need several lines to compute this)\n",
    "\n",
    "print 'El número de variables seleccionadas son: ' + str(len(pos_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "import numpy as np\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(sum(pos_sel), 178643, 'incorrect result: test error is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the number of selected features depends on the C value. Complete the next cell to select this value by cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "#################################################\n",
    "\n",
    "# Initialize variables \n",
    "bestModel = # FILL IN\n",
    "best_error = # FILL IN\n",
    "best_C = # FILL IN\n",
    "# Range of depth values to explore\n",
    "C_params = [0.01, 0.1, 1]\n",
    "\n",
    "for C_value in C_params:\n",
    "    # Train the 1 vs. all SVM models (set regularization parameter to C_value)\n",
    "    multiclass_SVM = train_1vsall_SVM(trainingData, regParam=C_value, regType='l1')\n",
    "    \n",
    "    # Compute the model error over the validation data \n",
    "    error_1vsall = # FILL IN\n",
    "    \n",
    "    # Compute the number of selected features\n",
    "    num_var_sel = # FILL IN\n",
    "\n",
    "    print 'C value is ' + str(C_value) + ', the number of selected features is ' + str(num_var_sel) + ' and the validation error is '+ str(error_1vsall) \n",
    "    \n",
    "    # If the error has reduced, save the model, the optimum depth and error in bestModel, best_depth and best_error variables\n",
    "    if (error_1vsall < best_error):\n",
    "            bestModel = # FILL IN\n",
    "            best_C = # FILL IN\n",
    "            best_error = # FILL IN\n",
    "            \n",
    "print 'Optimum C value is: ' + str(best_C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
