{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification II Lab: Working with classifiers**\n",
    "\n",
    "Author: Vanessa GÃ³mez Verdejo (http://vanessa.webs.tsc.uc3m.es/)\n",
    "\n",
    "Updated: 07/02/2017 (working with sklearn 0.18.1)\n",
    "\n",
    "In this lab session we are going to deep in our knowledge about classifiers by managing most well-known classification algorithms. Besides, we are going to review some useful techniques, such as the cross validation process, which will allow us to adjust the free parameters of the classifier. For this lab session, we will work with a real multiclass data set: Iris. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### * Part 1: K-Nearest Neighbours (K-NN)*\n",
    "#### * Part 2: Support Vector Machines (SVMs) with different kernel funcions*\n",
    "#### * Part 3: Other linear classifiers*\n",
    "\n",
    "\n",
    "As in previous lab session, to implement the different approaches we will base in [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 0: Load and prepare the data **\n",
    "\n",
    "The [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) consists of 150 patterns corresponding to 3 different types of irises: Setosa, Versicolour, and Virginica. Each pattern contains the sepal and petal lengths and widths. Despite having four input features, for display purposes, we are going to start working with the first two features: sepal length and sepal width. \n",
    "\n",
    "The below code let you:\n",
    "* create training and testing partitions with the 60% and 40% of the original data\n",
    "* normalize the data to zero mean and unitary standard deviation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load Iris data set\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[ 1, 3]]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "# Create data partitions\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.6)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: K-NN**\n",
    "\n",
    "A K-NN approach classifies each new data searching its K nearest neighbors (among the training data) and assigning the majority class among these neighbors. As expected, its performance depends on the number of neighbors (K) used.\n",
    "\n",
    "#### ** 1.1: Training a K-NN classifier**\n",
    "To start to work, let's analyze for different values of K the K-NN performance, both over training and test sets. Use the [KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) method to complete the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "K_max=20\n",
    "rang_K = np.arange(1, K_max+1)\n",
    "vect_tr=[]\n",
    "vect_test=[]\n",
    "\n",
    "for n_neighbors in rang_K:\n",
    "    # Create a KNN classifier, train it and compute training and error accuracies.\n",
    "    clf = #<FILL IN>\n",
    "    acc_tr = #<FILL IN>\n",
    "    acc_test = #<FILL IN>\n",
    "    \n",
    "    # Saving accuracies in a list \n",
    "    vect_tr#<FILL IN>\n",
    "    vect_test#<FILL IN>\n",
    "\n",
    "    print(\"For K = %d, train accuracy is %2.2f%% and test accuracy is %2.2f%%\"\n",
    "          % (n_neighbors, 100*acc_tr, 100*acc_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(np.sum(np.array(vect_tr)),2), 18.90, 'incorrect result: Training error of K-NN is uncorrect')\n",
    "Test.assertEquals(np.round(np.sum(np.array(vect_test)),2), 18.36, 'incorrect result: Test error of K-NN is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next code, let you plot the evolution of above computed train and test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot acc vs K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rang_K,vect_tr,'b', label='Training accuracy')\n",
    "plt.plot(rang_K,vect_test,'r', label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('K value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evolution of K-NN accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure points out the necessity of selecting the adequate value of K. And, as expected, using the training error for such selection would provide a poor generalization.\n",
    "\n",
    "#### ** 1.2: Selecting the number of neighbors of a K-NN classifier**\n",
    "\n",
    "Therefore, next step will consist of applying a cross validation (CV) process to select the optimum value of K. You can use the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function to implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters\n",
    "K_max = 20\n",
    "rang_K = np.arange(1, K_max+1)\n",
    "nfold = 10\n",
    "# Define a dictionary with the name of the parameters to explore as a key and the ranges to explores as value\n",
    "tuned_parameters = [{'n_neighbors': rang_K}]\n",
    "\n",
    "\n",
    "# Cross validation proccess \n",
    "clf_base = neighbors.KNeighborsClassifier( )\n",
    "# Define the classfifier with the CV process (use GridSearchCV here!!!)\n",
    "clf = #<FILL IN>\n",
    "# Train it (this executes the CV)\n",
    "clf.#<FILL IN>\n",
    "\n",
    "print 'CV process sucessfully finished'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the CV process, the classifier object  contains the information of the CV process (next cell explore the parameter \".grid\\_scores\\_\" to obtain this information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing results\n",
    "print(\"Cross validation results:\")\n",
    "\n",
    "paramsFolds = clf.cv_results_['params']\n",
    "meanScoreFolds = clf.cv_results_['mean_test_score']\n",
    "stdScoreFolds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for fold in range(nfold):\n",
    "    params = paramsFolds[fold]\n",
    "    mean_score = meanScoreFolds[fold]\n",
    "    std_score = stdScoreFolds[fold]\n",
    "    print(\"For K = %d, validation accuracy is %2.2f (+/-%1.3f)%%\" \n",
    "          % (params['n_neighbors'], 100*mean_score, 100*std_score / 2))\n",
    "\n",
    "# Selecting validation error (mean values)\n",
    "vect_val=meanScoreFolds\n",
    "\n",
    "# Ploting results\n",
    "plt.figure()\n",
    "plt.plot(rang_K,vect_tr,'b', label='Training accuracy')\n",
    "plt.plot(rang_K,vect_test,'r', label='Test accuracy')\n",
    "plt.plot(rang_K,vect_val,'g', label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('K value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evolution of K-NN accuracy (including validation result)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the fields \".best\\_estimator\\_\" and \".best\\_params\\_\" of the classifier generated by the CV process:\n",
    "* \".best\\_estimator\\_\" contains  the final classifier trained with this select value.\n",
    "* \".best\\_params\\_\" is a dictionary with the selected parameters. In our example, \"best\\_params\\_['n\\_neighbors']\" would provide the selected value of K.\n",
    "\n",
    "Save the selected value of K in variable denoted \"K_opt\" and compute the test error of the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Assign to K_opt the value of K selected by CV\n",
    "K_opt = # <FILL IN>\n",
    "print(\"The value optimum of K is %d\" %(K_opt))\n",
    "\n",
    "# Select the final classifier  and compute its test error\n",
    "KNN_acc_test = # <FILL IN>\n",
    "print(\"The test accuracy is %2.2f\" %(100*KNN_acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also compute the test error directly over the classifier object return by the CV process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KNN_acc_test2 = clf.score(X_test, Y_test)\n",
    "print(\"The test accuracy is %2.2f\" %(100*KNN_acc_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(K_opt, 6, 'incorrect result: The value of K_opt is uncorrect')\n",
    "Test.assertEquals(np.round(KNN_acc_test,4), 0.9444, 'incorrect result: Test error of K-NN after CV process is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â ** Advanced work**\n",
    "\n",
    " Complete the following code to implement the CV process without using GridSearchCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Parameters\n",
    "K_max = 20\n",
    "rang_K = np.arange(1, K_max+1)\n",
    "nfold = 10\n",
    "N_tr, N_dim = X_train.shape\n",
    "\n",
    "# Create data partitions (check the structure of folds)\n",
    "skf = StratifiedKFold(n_splits=nfold)\n",
    "\n",
    "# Create variable (acc_val) to save the validation results\n",
    "acc_val = np.zeros([nfold, K_max])\n",
    "for f, (train, val) in enumerate(skf.split(X_train, Y_train)):\n",
    "    # f is an index with the number of fold\n",
    "    # train has the training data positions for fold f\n",
    "    # val has the validation data positions for fold f\n",
    "    \n",
    "    # Create training and validation partitions for fold f\n",
    "    X_tr_f = # <FILL IN>\n",
    "    Y_tr_f = # <FILL IN>\n",
    "    X_val_f = # <FILL IN>\n",
    "    Y_val_f = # <FILL IN>\n",
    "\n",
    "    # Explore the values of K\n",
    "    for K, n_neighbors in enumerate(rang_K):\n",
    "        # Train a K-NN with the training data of this fold data using n_neighbors\n",
    "        clf = # <FILL IN>\n",
    "        # Evaluate its accuaracy over the validation data of this fold and save it in acc_val\n",
    "        acc_val[f, K] = # <FILL IN>\n",
    "\n",
    "# Compute the average validation error (average over the different folds)\n",
    "acc_val_med = # <FILL IN>\n",
    "# Select the optimum value of K as the value which achieves the maximum average validation error\n",
    "K_opt = # <FILL IN>\n",
    "\n",
    "# Train the final K-NN classfier (with K_opt) and obtain its test error\n",
    "clf = # <FILL IN>\n",
    "acc_test = # <FILL IN>\n",
    "\n",
    "print \"K_NN with optimum K = \" + np.array_str(K_opt) + \\\n",
    "    \". Test error = \" + np.array_str(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(K_opt, 6, 'incorrect result: The value of K_opt is uncorrect')\n",
    "Test.assertEquals(np.round(KNN_acc_test,4), 0.9444, 'incorrect result: Test error of K-NN after CV process is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 1.3: Plotting K-NN classification boundary**\n",
    "\n",
    "To finish this section, we are going to plot classification boundary of the K-NN classifier. For this purpose, next cell contains the plot_boundary function used in the first lab session. Use this function to plot the classification boundary of the cross validated K-NN classifier over training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_boundary(clf, X, Y, plt):\n",
    "    \"\"\"Plot the classification regions for a given classifier.\n",
    "\n",
    "    Args:\n",
    "        clf: scikit-learn classifier object.\n",
    "        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only frist two \n",
    "                            dimensions are ploted\n",
    "        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n",
    "        plt: graphic object where you wish to plot                                             \n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    plot_colors = \"brymc\"\n",
    "    plot_step = 0.02\n",
    "    n_classes = np.unique(Y).shape[0]\n",
    "    # Plot the decision regions\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                        np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(Y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "# Plot classification boundary over training data\n",
    "plt.subplot(1,2,1)\n",
    "plot_boundary(clf, X_train, Y_train, plt)\n",
    "\n",
    "# Plot classification boundary over test data\n",
    "plt.subplot(1,2,2)\n",
    "plot_boundary(clf, X_test, Y_test, plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: SVM**\n",
    "\n",
    "SVM is one of the most well-known classifiers due to its good generalization properties in many different applications. Besides, by means of the kernel trick, its linear formulation can easily extended to a non linear fashion. \n",
    "\n",
    "Here, we will test its performance when different kernel functions are used. For this purpose, we can use the [SCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) method, which let you select the kernel function to be used, and the method GridSearchCV( ) to adjust the different free parameters (C and kernel parameter). \n",
    "\n",
    "Complete the following cells, when it is required, to train in each case a linear SVM (defining kernel='linear' in the method SCV( )), an SVM with gaussian kernel (kernel='rbf') and an SVM with polynomial kernel (kernel='poly'). \n",
    "\n",
    "For each method, adjust the corresponding free parameters with a 10 fold CV process (the ranges to explore are defined at the beginning of each cell). Return the values of selected parameters and the accuracy of the final SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.1: Linear SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import svm\n",
    "rang_C = np.logspace(-3, 3, 10)\n",
    "tuned_parameters = [{'C': rang_C}]\n",
    "\n",
    "nfold = 10\n",
    "\n",
    "# Train a liner SVM and adjust by CV the parameter C\n",
    "clf_base = svm.SVC(kernel='linear')\n",
    "lin_svc  = # <FILL IN> \n",
    "lin_svc.# <FILL IN> \n",
    "\n",
    "# Save the value of C selected and compute the final accuracy\n",
    "C_opt = # <FILL IN> \n",
    "acc_lin_svc = # <FILL IN> \n",
    "\n",
    "print \"The C value selected is \" + str(C_opt)\n",
    "print(\"The test accuracy of the linear SVM is %2.2f\" %(100*acc_lin_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(C_opt,2), 0.46, 'incorrect result: The value of C_opt is uncorrect')\n",
    "Test.assertEquals(np.round(acc_lin_svc,4), 0.9556, 'incorrect result: Test accuracy of the linear SVM after CV process is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.2: SVM with gaussian kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "n_dim=X_train.shape[1]\n",
    "rang_g=np.array([0.125, 0.25, 0.5, 1, 2, 4, 8])/(np.sqrt(n_dim))\n",
    "tuned_parameters = [{'C': rang_C, 'gamma': rang_g}]\n",
    "\n",
    "# Train an SVM with gaussian kernel and adjust by CV the parameter C\n",
    "clf_base = svm.SVC(kernel='rbf')\n",
    "rbf_svc  = # <FILL IN> \n",
    "rbf_svc. # <FILL IN> \n",
    "# Save the values of C and gamma selected and compute the final accuracy\n",
    "C_opt = # <FILL IN> \n",
    "g_opt = # <FILL IN> \n",
    "\n",
    "\n",
    "print \"The C value selected is \" + str(C_opt)\n",
    "print \"The gamma value selected is \" + str(g_opt)\n",
    "acc_rbf_svc = rbf_svc.score(X_test, Y_test)\n",
    "print(\"The test accuracy of the RBF SVM is %2.2f\" %(100*acc_rbf_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(C_opt, 2), 2.15, 'incorrect result: The value of C_opt is uncorrect')\n",
    "Test.assertEquals(np.round(g_opt, 2), 0.09, 'incorrect result: The value of g_opt is uncorrect')\n",
    "Test.assertEquals(np.round(acc_rbf_svc, 4), 0.9444, 'incorrect result: Test accuracy of the rbf SVM after CV process is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.3. SVM with polynomial kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "rang_d=np.arange(1,5)\n",
    "tuned_parameters = [{'C': rang_C, 'degree': rang_d}]\n",
    "\n",
    "# Train an SVM with polynomial kernel and adjust by CV the parameter C\n",
    "clf_base =  svm.SVC(kernel='poly')\n",
    "poly_svc  = # <FILL IN> \n",
    "poly_svc.# <FILL IN> \n",
    "\n",
    "# Save the values of C and degree selected and compute the final accuracy\n",
    "C_opt = # <FILL IN> \n",
    "d_opt = # <FILL IN> \n",
    "\n",
    "\n",
    "print \"The C value selected is \" + str(C_opt)\n",
    "print \"The degree value selected is \" + str(d_opt)\n",
    "acc_poly_svc = poly_svc.score(X_test, Y_test)\n",
    "print(\"The test accuracy of the polynomial SVM is %2.2f\" %(100*acc_poly_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(C_opt, 2), 10 , 'incorrect result: The value of C_opt is uncorrect')\n",
    "Test.assertEquals(np.round(d_opt, 2), 3, 'incorrect result: The value of d_opt is uncorrect')\n",
    "Test.assertEquals(np.round(acc_poly_svc, 4), 0.9111, 'incorrect result: Test accuracy of the polynomial SVM after CV process is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.4. Plot the classification boundaries and support vectors**\n",
    "\n",
    "The SVM decision function depends on some subset of the training data, called the support vectors. In this section we are going to compare the boundaries provided by the different kernel functions and study which training points are support vectors.\n",
    "\n",
    "As in previous sections, you can use the plot_boundary( ) function to display the decision regions. To obtain the support vectors, you can access to the parameters \"support\\_vectors\\_\" of the svm classifier object. Add the necessary code to include them in the figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "plt.figure(1,figsize=(10, 4))\n",
    "\n",
    "# Linear SVM \n",
    "plt.subplot(1,2,1)\n",
    "plot_boundary(lin_svc , X_train, Y_train, plt)\n",
    "plt.title ('SVM boundary and training data')\n",
    "plt.subplot(1,2,2)\n",
    "plot_boundary(lin_svc , X_train, Y_train, plt)\n",
    "SVs=# <FILL IN> \n",
    "plt.scatter(# <FILL IN> )\n",
    "plt.title ('SVM boundary, training data and SVs')\n",
    "\n",
    "\n",
    "plt.figure(2,figsize=(10, 4))\n",
    "# RBF SVM \n",
    "plt.subplot(1,2,1)\n",
    "plot_boundary(rbf_svc , X_train, Y_train, plt)\n",
    "plt.title ('SVM boundary and training data')\n",
    "plt.subplot(1,2,2)\n",
    "plot_boundary(rbf_svc , X_train, Y_train, plt)\n",
    "SVs=# <FILL IN> \n",
    "plt.scatter(# <FILL IN> )\n",
    "plt.title ('SVM boundary, training data and SVs')\n",
    "\n",
    "\n",
    "plt.figure(3,figsize=(10, 4))\n",
    "# Polynomial SVM \n",
    "plt.subplot(1,2,1)\n",
    "plot_boundary(poly_svc , X_train, Y_train, plt)\n",
    "plt.title ('SVM boundary and training data')\n",
    "plt.subplot(1,2,2)\n",
    "plot_boundary(poly_svc , X_train, Y_train, plt)\n",
    "SVs=# <FILL IN> \n",
    "plt.scatter(# <FILL IN> )\n",
    "plt.title ('SVM boundary, training data and SVs')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the boundaries of linear and RBF SVMs and,  even, their SVs are quite similar. Examine the value selected for the parameter gamma of the gaussian kernel and try to explain because RBF SVM boundary tends to be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: other linear classifiers **\n",
    "\n",
    "Include the necessary code to train and test a classifier based in:\n",
    "1. A logistic regression model: in thiscase adjust the C parameter by CV\n",
    "2. Linear Discrimation Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rang_C = np.logspace(-3, 3, 10)\n",
    "tuned_parameters = [{'C': rang_C}]\n",
    "nfold = 10\n",
    "\n",
    "# Train a LR model and adjust by CV the parameter C\n",
    "clf_LR  = GridSearchCV(LogisticRegression(),\n",
    "                   tuned_parameters, cv=nfold)\n",
    "clf_LR.# <FILL IN> \n",
    "acc_test_LR=# <FILL IN> \n",
    "\n",
    "# LDA \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "clf_LDA = LDA()\n",
    "clf_LDA.# <FILL IN> \n",
    "acc_test_LDA=# <FILL IN> \n",
    "\n",
    "print(\"The test accuracy of LR is %2.2f\" %(100*acc_test_LR))\n",
    "print(\"The test accuracy of LDA is %2.2f\" %(100*acc_test_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(100*acc_test_LR, 2), 87.78 , 'incorrect result: The value of acc_test_LR is uncorrect')\n",
    "Test.assertEquals(np.round(100*acc_test_LDA, 2), 96.67, 'incorrect result: The value of acc_test_LDA is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the necessary code to plot their classification boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "plt.figure(1,figsize=(10, 4))\n",
    "\n",
    "# LR\n",
    "plt.subplot(1,2,1)\n",
    "plot_boundary(# <FILL IN> )\n",
    "plt.title ('LR')\n",
    "# LDA\n",
    "plt.subplot(1,2,2)\n",
    "plot_boundary(# <FILL IN> )\n",
    "plt.title ('LDA')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
