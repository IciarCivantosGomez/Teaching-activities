{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification III Lab: Working with classifiers**\n",
    "\n",
    "In this lab session we are going to continue working with classification algorithms, mainly, we are going to focus on decision trees and their use in ensembles. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### * Part 1: Trees*\n",
    "#### * Part 2: Random forests*\n",
    "#### * Part 3: Ensembles of classifiers: bagging and boosting*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 0: Load and preprocess data**\n",
    "\n",
    "In the following sections, we are going to use all input features of the Iris dataset. So, let's start running the following cell to load the complete Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data # All input features are used\n",
    "Y = iris.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Trees**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 1.1: Training a decision Tree**\n",
    "\n",
    "Decision Trees learn simple decision rules selecting iteratively a input feature and setting a threshold over it, so the are simple tool to understand and to interpret.\n",
    "\n",
    "Use the [DecisionTreeClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) function to train a decision tree. Although the tree depth is usually a parameter to select, here we are working with only for input features, so you can use all default parameter and obtain a good performance. Complete the following code to return in the variable acc\\_tree the tree accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import tree\n",
    "clf_tree = # <FILL IN>\n",
    "\n",
    "acc_tree= # <FILL IN>\n",
    "\n",
    "print(\"The test accuracy of the decision tree is %2.2f\" %(100*acc_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(acc_tree, 2), 0.95 , 'incorrect result: The value of C_opt is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 1.2: Analyzing the results**\n",
    "\n",
    "The tree module, let us analyze the structure of the trained tree. For this purpose, you may need install some libraries (in class computers is alredy installed). \n",
    "\n",
    "Please, follow these steps:\n",
    "\n",
    "1. Check that you are working with scikit-learn 0.17. You can checked this from a python terminal:\n",
    "        >> import sklearn\n",
    "        >> sklearn.\\__version\\__\n",
    "    \n",
    "   If it is necessary, you can upgrade sklearn from a terminal with: \n",
    "       >> pip install --upgrade scikit-learn\n",
    "   \n",
    "2. Go to a terminal and run the following command lines:\n",
    "\n",
    "   2.A. For MAC user:\n",
    "        >> pip uninstall pyparsing\n",
    "        >> pip install -Iv https://pypi.python.org/packages/source/p/pyparsing/pyparsing-1.5.7.tar.gz#md5=9be0fcdcc595199c646ab317c1d9a709\n",
    "        >> pip install pydot\n",
    "        >> brew install graphviz\n",
    "        \n",
    "   2.B. For Linux users:\n",
    "       * Install Graphviz:\n",
    "           ** (In Fedora / Centos)  >> sudo dnf -y install graphviz\n",
    "           ** (Ubuntu / Debian) >> sudo apt-get install graphviz\n",
    "       >> conda update scikit-learn\n",
    "       >> pip install --upgrade pip\n",
    "       >> pip uninstall pyparsing\n",
    "       >> pip install -Iv https://pypi.python.org/packages/source/p/pyparsing/pyparsing-1.5.7.tar.gz#md5=9be0fcdcc595199c646ab317c1d9a709\n",
    "       >> pip install graphviz\n",
    "       >> pip install pydot\n",
    "       \n",
    "You have to restart the notebook kernel to make this changes work. Then, run the following cell to visualize the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "import pydot\n",
    "from IPython.display import Image  \n",
    "\n",
    "dot_data = StringIO()  \n",
    "tree.export_graphviz(clf_tree, out_file=dot_data,  \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use the following [example](http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#example-tree-plot-iris-py) of the scikit-learn help, to plot the classification regions for different pairs of input features. Modify the necessary code line to plot our training data over the decision regions.\n",
    "\n",
    "Be careful, this examples retrains different classifiers for each pair of input features; therefore, its solution differs from the above one that we have just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Random Forest**\n",
    "\n",
    "#### ** 2.1: Training a Random Forest**\n",
    "\n",
    "A Random Forest (RF) trains several decision tree classifiers, where each one is trained with different sub-samples of the training data, and averages their outputs to improve the final accuracy.\n",
    "\n",
    "Use the [RandomForestClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) function to train a RF classifier and select by cross validation the number of trees to use. The remaining parameters, such as the number of subsampled data or features, can be used with their default values. Return the optimal number of trees to be used and the final accuracy of the RF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rang_n_trees=np.arange(1,10)\n",
    "tuned_parameters = [{'n_estimators': rang_n_trees}]\n",
    "nfold = 10\n",
    "\n",
    "clf_RF  = #<FILL IN>\n",
    "n_trees_opt = #<FILL IN>\n",
    "acc_RF = #<FILL IN>\n",
    "\n",
    "print \"The number of selected trees is \" + str(n_trees_opt)\n",
    "print(\"The test accuracy of the RF is %2.2f\" %(100*acc_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code again, do you obtain the same accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.2: Obtaining results statistically significant**\n",
    "\n",
    "Random forest have a random component when the training data are subsampled, so you can obtain a different result for different runnings of the algorithm. In this case, to be able to provide a statistically significant measurement of the performance of the classifier, we need to average the result over a large number of runs.\n",
    "\n",
    "Complete the following code, to train again the RF classifier, but averaging its test accuracies over 50 runs. Provide its average accuracy and the average number of selected trees (include their standard deviations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "print 'This can take a some minutes, be patient'\n",
    "# Create RF classifier object with CV\n",
    "clf_RF  = # <FILL IN> \n",
    "\n",
    "acc_RF_vector=[]\n",
    "n_trees_vector=[]\n",
    "for run in np.arange(50):\n",
    "    # For each run, train it, compute its accuracy and examine the number of optimal trees\n",
    "    clf_RF.# <FILL IN> \n",
    "    acc = # <FILL IN> \n",
    "    acc_RF_vector.append(acc)\n",
    "    n_trees = # <FILL IN> \n",
    "    n_trees_vector.append(n_trees)\n",
    "\n",
    "# Compute averaged accuracies and number of used trees\n",
    "mean_acc_RF = # <FILL IN> \n",
    "std_acc_RF = # <FILL IN> \n",
    "\n",
    "mean_n_trees = # <FILL IN> \n",
    "std_n_trees = # <FILL IN> \n",
    "\n",
    "# Print the results\n",
    "print('Averaged accuracy for RF classifier is %2.2f +/- %2.2f '%(100*mean_acc_RF, 100*std_acc_RF))\n",
    "print('Averaged number of selected trees is %2.2f +/- %2.2f '%(mean_n_trees, std_n_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(np.round(mean_acc_RF, 1), 0.9 , 'incorrect result: The value of mean_acc_RF is uncorrect')\n",
    "Test.assertEquals(np.round(std_acc_RF, 2), 0.03 , 'incorrect result: The value of std_acc_RF is uncorrect')\n",
    "Test.assertEquals(np.round(mean_n_trees, 1), 4.2 , 'incorrect result: The value of mean_n_trees is uncorrect')\n",
    "Test.assertEquals(np.round(std_n_trees, 1), 2.0 , 'incorrect result: The value of std_n_trees is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Ensembles**\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators or learners to obtain a classifier of improved performance. We are going to work with two ensemble methods:\n",
    "\n",
    "* Bagging methods: their driving principle is to build several estimators with diversity among them and then to average their predictions. \n",
    "* Boosting methods: in this case, base estimators are built sequentially forcing new learners to pay more attention to samples missclassified by previous learners. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 3.1. Bagging methods**\n",
    "\n",
    "Here, to implement bagged classifiers, we are going to use [BaggingClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier) object which includes different degrees of freedom in the learners design: with or without samples replacement, selecting random subsets of features instead of samples or selecting subsets of both samples and features. \n",
    "\n",
    "For the sake of simplicity, we are going to use as base learner a decision stump (i.e., a decision tree with one depth level). Note that in the case of using decision trees as learners, the resulting ensemble results in a random forest. \n",
    "\n",
    "Complete the following code to train a ensemble of bagged decision stumps. Set max\\_samples (percentage of training data used to train each learner) and max\\_features parameters (percentage of input features used to train each learner) to 0.5, and fix to 10 the number of learners used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "base_learner = tree.DecisionTreeClassifier(max_depth=1)\n",
    "bagging = BaggingClassifier(base_learner, n_estimators = 10, max_samples=0.5, max_features = 0.5)\n",
    "bagging.fit(X_train, Y_train)\n",
    "acc_test = bagging.score(X_test, Y_test)\n",
    "\n",
    "print('Accuracy of bagged ensemble is %2.2f '%(100*acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the final ensemble performance according to the number of learners. Average the result over 20 or more different runs to obtain statically significant results (note that the above accuracy change if you run the code again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Initialize the random generator seed to test results\n",
    "np.random.seed(0)\n",
    "\n",
    "acc_test_evol = []\n",
    "rang_n_learners = range(1,50,2)\n",
    "for n_learners in rang_n_learners:\n",
    "    acc_test_run=[]\n",
    "    for run in range(50):\n",
    "        bagging = # <FILL IN>\n",
    "        acc = # <FILL IN>\n",
    "        acc_test_run.append(acc)\n",
    "    acc_test_evol.append(np.mean(acc_test_run))\n",
    "\n",
    "# Ploting results\n",
    "plt.figure()\n",
    "plt.plot(rang_n_learners,acc_test_evol)\n",
    "plt.xlabel('Number of learners')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evolution of the bagged ensemble accuracy with the number of learners ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(acc_test_evol[-1], 2), 0.94 , 'incorrect result: The value final of acc_test_evol is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 3.2. Adaboost**\n",
    "\n",
    "To train an AdaBoost classifier, scikit-learn provides [AdaBoostClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) method which includes two versions of the Adaboost algorithm:\n",
    "* Discrete Adaboost: the learners' outputs are discretized (they provide an estimation of the labels).\n",
    "* Real Adaboost: the learners' outputs are real values (they are the soft-outputs or the class probabilities).\n",
    "\n",
    "As in previous subsection, use a decision stump as base learner. Fix to 50 the number of learners and compare the results of both approaches: Discrete Adaboost (set algorithm parameter to 'SAMME') and Real Adaboost (algorithm='SAMME.R')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Initialize the random generator seed to test results\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "base_learner = tree.DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Train a discrete Adaboost classifier and obtain its accuracy\n",
    "AB_D = #<FILL IN>\n",
    "acc_AB_D = # <FILL IN>\n",
    "\n",
    "# Train a real Adaboost classifier and obtain its accuracy\n",
    "AB_R = # <FILL IN>\n",
    "acc_AB_R = # <FILL IN>\n",
    "\n",
    "print('Accuracy of discrete adaboost ensemble is %2.2f '%(100*acc_AB_D))\n",
    "print('Accuracy of real adaboost ensemble is %2.2f '%(100*acc_AB_R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST accuracy values\n",
    "Test.assertEquals(np.round(acc_AB_D, 2), 0.95 , 'incorrect result: The value of acc_AB_D is uncorrect')\n",
    "Test.assertEquals(np.round(acc_AB_R, 2), 0.88 , 'incorrect result: The value of acc_AB_R is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike BaggingClassifier() method, AdaBoostClassifier() let you analyze the evolution of error without having to train the ensemble for different number of learners. For this task, you can use the classifier method .staged_score() which returns the evolution of the ensemble accuracy. Note that it returns this information with a generator object, so you have to iterate over it to access to each element.\n",
    "\n",
    "The following code lines let you plot the evolution of the ensemble accuracy (over the test data) for both discrete and real Adaboost approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_AB_D_evol=[acc for acc in AB_D.staged_score(X_test, Y_test)]\n",
    "acc_AB_R_evol=[acc for acc in AB_R.staged_score(X_test, Y_test)]\n",
    "\n",
    "\n",
    "# Ploting results\n",
    "rang_n_learners=np.arange(50)+1\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(rang_n_learners,acc_AB_D_evol)\n",
    "plt.xlabel('Number of learners')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Discrete AB accuracy')\n",
    "plt.subplot(212)\n",
    "plt.plot(rang_n_learners,acc_AB_R_evol)\n",
    "plt.xlabel('Number of learners')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Real AB accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can check the following scikit-learn [example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html) where the performance of different ensembles over the Iris dataset is analyzed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
