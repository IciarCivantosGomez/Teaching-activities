{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab Session : Feature extraction II**\n",
    "\n",
    "Author: Vanessa GÃ³mez Verdejo (http://vanessa.webs.tsc.uc3m.es/)\n",
    "\n",
    "Updated: 27/02/2017 (working with sklearn 0.18.1)\n",
    "\n",
    "In this lab session we are going to work with some of the kernelized extensions of most well-known feature extraction techniques: PCA, PLS and CCA.\n",
    "\n",
    "As in the previous notebook, to analyze the discriminatory capability of the extracted features, let's use a linear SVM as classifier and use its final accuracy over the test data to evaluate the goodness of the different feature extraction methods.\n",
    "\n",
    "To implement the different approaches we will base on [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "#### *Part 2: Non linear feature selection* \n",
    "##### *   Part 2.1: Kernel extensions of PCA*\n",
    "##### *   Part 2.2: Analyzing the influence of the kernel parameter*\n",
    "##### *  Part 2.3: Kernel MVA approaches*\n",
    "\n",
    "As you progress in this notebook, you will have to complete some exercises. Each exercise includes an explanation of what is expected, followed by code cells where one or several lines will have written down `<FILL IN>`.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line.  Once the `<FILL IN>` sections are updated and the code can be run; below this cell, you will find the test cell (beginning with the line `# TEST CELL`) and you can run it to verify the correctness of your solution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part 2: Non linear feature selection* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** 2.0: Creating toy problem **\n",
    "\n",
    "The following code let you generate a bidimensional problem consisting of thee circles of data with different radius, each one associated to a different class. \n",
    "\n",
    "As expected from the geometry of the problem, the classification boundary is not linear, so we will able to analyze the advantages of using no linear feature extraction techniques to transform the input space to a new space where a linear classifier can provide an accurate solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "X, Y = make_circles(n_samples=400, factor=.6, noise=.1)\n",
    "\n",
    "X_c2 = 0.1*np.random.randn(200,2)\n",
    "Y_c2 = 2*np.ones((200,))\n",
    "\n",
    "X= np.vstack([X,X_c2])\n",
    "Y= np.hstack([Y,Y_c2])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Original space\")\n",
    "reds = Y == 0\n",
    "blues = Y == 1\n",
    "green = Y == 2\n",
    "\n",
    "plt.plot(X[reds, 0], X[reds, 1], \"ro\")\n",
    "plt.plot(X[blues, 0], X[blues, 1], \"bo\")\n",
    "plt.plot(X[green, 0], X[green, 1], \"go\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Binarize the labels for supervised feature extraction methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "Y_test_bin = label_binarize(Y_test, classes=set_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2.1: Kernel PCA**\n",
    "\n",
    "To extend the previous PCA feature extraction approach to its non-linear version, we can use of [KernelPCA( )](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA) function. \n",
    "\n",
    "Let's start this section computing the different kernel matrix that we need to train and evaluate the different feature extraction methods. For this exercise, we are going to consider a Radial Basis Function kernel (RBF), where each element of the kernel matrix is given by $k(x_i,x_j) = \\exp (- \\gamma (x_i -x_j)^2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the advantages of the non linear feature extraction, let's compare it with its linear version. So, let's start computing both linear and kernelized versions of PCA. Complete the following code to obtain the variables (P_train, P_test) and (P_train_k, P_test_k) which have to contain, respectively, the projected data of the linear PCA and the KPCA.\n",
    "\n",
    "To start to work, compute a maximum of two new projected features and fix gamma (the kernel parameter) to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "N_feat_max=2\n",
    "\n",
    "# linear PCA\n",
    "pca = PCA(n_components=N_feat_max)\n",
    "pca.fit(# <FILL IN>)\n",
    "P_train = # <FILL IN>\n",
    "P_test = # <FILL IN>\n",
    "\n",
    "# KPCA\n",
    "pca_K = KernelPCA(n_components=N_feat_max, kernel=\"rbf\", gamma=1)\n",
    "pca_K.fit(# <FILL IN>)\n",
    "P_train_k = # <FILL IN>\n",
    "P_test_k =# <FILL IN>\n",
    "\n",
    "print 'PCA and KPCA projections sucessfully computed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the discriminatory capability of the projected data (both linear and kernelized ones) feeding with them a linear SVM and measuring its accuracy over the test data. Complete the following to code to return in variables acc_test_lin and acc_test_kernel the SVM test accuracy using either the linear PCA projected data or the KPCA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Define SVM classifier\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train it using linear PCA projections and evaluate it\n",
    "clf.fit(#<FILL IN>)\n",
    "acc_test_lin = #<FILL IN>\n",
    "\n",
    "# Train it using KPCA projections and evaluate it\n",
    "clf.fit(#<FILL IN>)\n",
    "acc_test_kernel = #<FILL IN>\n",
    "\n",
    "print(\"The test accuracy using linear PCA projections is  %2.2f%%\" %(100*acc_test_lin))\n",
    "print(\"The test accuracy using KPCA projections is  %2.2f%%\" %(100*acc_test_kernel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.round(acc_test_lin,4), 0.2400, 'incorrect result: test accuracy using linear PCA projections is uncorrect')\n",
    "Test.assertEquals(np.round(acc_test_kernel,4), 0.9533, 'incorrect result: test accuracy using KPCA projections is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's analyze the transformation capabilities of the projected data using a KPCA vs. linear PCA plotting the resulting projected data for both training and test data sets.\n",
    "\n",
    "Just run the following cells to obtain the desired representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_projected_data(data, label):\n",
    "    \n",
    "    \"\"\"Plot the desired sample data assigning differenet colors according to their categories.\n",
    "    Only two first dimensions of data ar plot and only three different categories are considered.\n",
    "\n",
    "    Args:\n",
    "        data: data set to be plot (number data x dimensions). \n",
    "        labes: target vector indicating the category of each data.\n",
    "    \"\"\"\n",
    "    \n",
    "    reds = label == 0\n",
    "    blues = label == 1\n",
    "    green = label == 2\n",
    "\n",
    "    plt.plot(data[reds, 0], data[reds, 1], \"ro\")\n",
    "    plt.plot(data[blues, 0], data[blues, 1], \"bo\")\n",
    "    plt.plot(data[green, 0], data[green, 1], \"go\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Projected space of linear PCA for training data\")\n",
    "plot_projected_data(P_train, Y_train)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Projected space of KPCA for training data\")\n",
    "plot_projected_data(P_train_k, Y_train)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"Projected space of linear PCA for test data\")\n",
    "plot_projected_data(P_test, Y_test)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"Projected space of KPCA for test data\")\n",
    "plot_projected_data(P_test_k, Y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the first cell and modify the kernel parameter (for instance, set gamma to 10 or 100) and run the code again. What is it happening? Why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2.2: Analyzing the influence of the kernel parameter**\n",
    "\n",
    "In the case of working with RBF kernel, the kernel width selection can be critical:\n",
    "* If gamma value is too high the width of the RBF is reduced (tending to be a delta function) and, therefore, the interaction between the training data is null. So we project each data over itself and assign it a dual variable in such a way that the best possible projection (for classification purposes) of the training data is obtain (causing overfitting problems).\n",
    "* If gamma value is close to zero, the RBF width increases and the kernel behavior tends to be similar to a linear kernel. In this case, the non-linear properties are lost.\n",
    "\n",
    "Therefore, in this kind of applications, the value of kernel width can be critical and it's advised selecting it by cross validation.\n",
    "\n",
    "This part of lab section aims to adjust the gamma parameter by a validation process. So, we will start creating a validation partition of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Redefine the data partitions: creating a validation partition\n",
    "\n",
    "# split training data into a training and validation set\n",
    "X_train2, X_val, Y_train2, Y_val = train_test_split(X_train, Y_train, test_size=0.33)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Binarize the training labels for supervised feature extraction methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin2 = label_binarize(Y_train2, classes=set_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the KPCA performance when different values of gamma are used. So, complete the below code in such a way that for each gamma value you can:\n",
    "* Train the KPCA and obtain the projections for the training, validation and test data.\n",
    "* Obtain the accuracies of a linear SVM over the validation and test partitions.\n",
    "\n",
    "Once, you have the validation and test accuracies for each gamma value, obtain the optimum gamma value (i.e., the gamma value which provides the maximum validation accuracy) and its corresponding test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Defining parameters\n",
    "N_feat_max = 2\n",
    "rang_g = [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50 , 100, 500, 1000]\n",
    "\n",
    "# Variables to save validation and test accuracies\n",
    "acc_val = []\n",
    "acc_test = []\n",
    "\n",
    "# Bucle to explore gamma values\n",
    "for g_value in rang_g:\n",
    "    print 'Evaluting with gamma ' + str(g_value)\n",
    "    \n",
    "    # 1. Train KPCA and project the data\n",
    "    pca_K = KernelPCA(#<FILL IN>)\n",
    "    pca_K.fit(#<FILL IN>)\n",
    "    P_train_k = #<FILL IN>\n",
    "    P_val_k = #<FILL IN>\n",
    "    P_test_k = #<FILL IN>\n",
    "        \n",
    "    # 2. Evaluate the projection performance\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(#<FILL IN>)\n",
    "    acc_val.append(#<FILL IN>)\n",
    "    acc_test.append(#<FILL IN>)\n",
    "\n",
    "# Find the optimum value of gamma and its corresponging test accuracy\n",
    "pos_max = #<FILL IN>\n",
    "g_opt = #<FILL IN>\n",
    "acc_test_opt = #<FILL IN>\n",
    "\n",
    "print 'Optimum of value of gamma: ' + str(g_opt)\n",
    "print 'Test accuracy: ' + str(acc_test_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(g_opt, 1, 'incorrect result: validated gamma value is uncorrect')\n",
    "Test.assertEquals(np.round(acc_test_opt,4), 0.9467, 'incorrect result: validated test accuracy is uncorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just run the next code to train the final model with the selected gamma value and plot the projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train KPCA and project the data\n",
    "pca_K = KernelPCA(n_components=N_feat_max, kernel=\"rbf\", gamma=g_opt)\n",
    "pca_K.fit(X_train2)\n",
    "P_train_k = pca_K.transform(X_train2)\n",
    "P_val_k = pca_K.transform(X_val)\n",
    "P_test_k = pca_K.transform(X_test)\n",
    "   \n",
    "# Plot the projected data\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Projected space of KPCA: train data\")\n",
    "plot_projected_data(P_train_k, Y_train2)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Projected space of KPCA: validation data\")\n",
    "plot_projected_data(P_val_k, Y_val)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Projected space of KPCA: test data\")\n",
    "plot_projected_data(P_test_k, Y_test)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 2.3: Kernel MVA approaches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have only used the KPCA approach because is the only not linear feature extraction method that it is included in Scikit-Learn. \n",
    "\n",
    "However, if we compare linear and kernel versions of MVA approaches, we could extend any linear MVA approach to its kernelized version. In this way, we can use the same methods reviewed for the linear approaches and extend them to its non-linear fashion calling it with the training kernel matrix, instead of the training data, and the method would learn the dual variables, instead of the eigenvectors.\n",
    "\n",
    "The following table relates both approaches:\n",
    "\n",
    "|                           |        Linear             |           Kernel           |\n",
    "|------                     |---------------------------|----------------------------|\n",
    "|Input data                 |      ${\\bf X}$            |         ${\\bf K}$          | \n",
    "|Variables to compute (fit) |Eigenvectors (${\\bf U}$)   |Dual variables (${\\bf A}$)  | \n",
    "|Projection vectors         |      ${\\bf U}$            |${\\bf U}=\\Phi^T {\\bf A}$ (cannot be computed) |\n",
    "|Project data (transform)   |${\\bf X}' = {\\bf U}^T {\\bf X}^T$|${\\bf X}' ={\\bf  A}^T \\Phi \\Phi^T = {\\bf  A}^T {\\bf K}$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Computing and centering kernel matrix **\n",
    "\n",
    "Let's start this section computing the different kernel matrix that we need to train and evaluate the different feature extraction methods. For this exercise, we are going to consider a Radial Basis Function kernel (RBF), where each element of the kernel matrix is given by $k(x_i,x_j) = \\exp (- \\gamma (x_i -x_j)^2)$.\n",
    "\n",
    "In particular, we need to compute two kernel matrix:\n",
    "* Training data kernel matrix (K_tr)  where the RBF is compute pairwise over the training data. The resulting matrix dimension is of $N_{tr} \\times N_{tr}$, being $N_{tr}$ the number of training data.\n",
    "* Test data kernel matrix (K_test) where the RBF is compute between training and test samples, i.e., in RBF expression the data $x_i$ belongs to test data whereas $x_j$ belongs to training data. The resulting matrix dimension is of $N_{test} \\times N_{tr}$, being $N_{test}$ and $N_{tr}$ the number of test and training data, respectively.\n",
    "\n",
    "Use the [rbf_kernel( )](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) function to compute the K_tr and K_test kernel matrix. Fix the kernel width value (gamma) to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Computing the kernel matrix\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "g_value = 1\n",
    "\n",
    "# Compute the kernel matrix (use the X_train matrix, before dividing it in validation and training data)\n",
    "K_tr = # <FILL IN>\n",
    "K_test  = # <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(K_tr.shape, (450,450), 'incorrect result: dimensions of training kernel matrix are uncorrect')\n",
    "Test.assertEquals(K_test.shape, (150,450), 'incorrect result: dimensions of test kernel matrix are uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compute these kernel matrix, they have to be centered (in the same way that we remove the mean when we work over the input space). For this purpose, next code provides you the function center_K(). Use it properly to remove the mean of both K_tr and K_test matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_K(K):\n",
    "    \"\"\"Center a kernel matrix K, i.e., removes the data mean in the feature space.\n",
    "\n",
    "    Args:\n",
    "        K: kernel matrix                                        \n",
    "    \"\"\"\n",
    "    size_1,size_2 = K.shape;\n",
    "    D1 = K.sum(axis=0)/size_1 \n",
    "    D2 = K.sum(axis=1)/size_2\n",
    "    E = D2.sum(axis=0)/size_1\n",
    "\n",
    "    K_n = K + np.tile(E,[size_1,size_2]) - np.tile(D1,[size_1,1]) - np.tile(D2,[size_2,1]).T\n",
    "    return K_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Center the kernel matrix\n",
    "K_tr_c = # <FILL IN>\n",
    "K_test_c = # <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TEST CELL\n",
    "###########################################################\n",
    "from test_helper import Test\n",
    "\n",
    "# TEST Training and test data generation\n",
    "Test.assertEquals(np.round(K_tr_c[0][0],2), 0.55, 'incorrect result: centered training kernel matrix is uncorrect')\n",
    "Test.assertEquals(np.round(K_test_c[0][0],2), -0.24, 'incorrect result: centered test kernel matrix is uncorrect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Alternative KPCA formulation **\n",
    "\n",
    "Complete the following code lines to obtain a KPCA implementaion using the linear PCA function and the kernel matrix as input data. Later, compare its result with that of the KPCA function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn import svm\n",
    "\n",
    "# Defining parameters\n",
    "N_feat_max = 2\n",
    "\n",
    "\n",
    "## PCA method (to complete)\n",
    "# 1. Train PCA with the kernel matrix and project the data\n",
    "pca_K2 = PCA(n_components=N_feat_max)\n",
    "pca_K2.#<FILL IN> \n",
    "P_train_k2 = #<FILL IN> \n",
    "P_test_k2 = #<FILL IN> \n",
    "        \n",
    "# 2. Evaluate the projection performance\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.#<FILL IN> \n",
    "print 'Test accuracy with PCA with a kenel matrix as input: '+ str(clf.score(P_test_k2, Y_test))\n",
    "\n",
    "## KPCA method (for comparison purposes)\n",
    "# 1. Train KPCA and project the data\n",
    "# Fixing gamma to 0.5 here, it is equivalent to gamma=1 in rbf function\n",
    "pca_K = KernelPCA(n_components=N_feat_max, kernel=\"rbf\", gamma=0.5) \n",
    "pca_K.fit(X_train)\n",
    "P_train_k = pca_K.transform(X_train)\n",
    "P_test_k = pca_K.transform(X_test)\n",
    "        \n",
    "# 2. Evaluate the projection performance\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(P_train_k, Y_train)\n",
    "print 'Test accuracy with KPCA: '+ str(clf.score(P_test_k, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Alternative KPLS  and KCCA formulations **\n",
    "\n",
    "Use the PLS and CCA methods with the kernel matrix to obtain no-linear (or kernelized) supervised feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
