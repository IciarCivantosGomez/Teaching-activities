{"cells":[{"cell_type":"markdown","source":["<H1> Deep Learning with TensorFlow </H1>"],"metadata":{}},{"cell_type":"markdown","source":["<h2> 1. Installing and importing TensorFlow </h2>\n\nAt first, we must install the TensorFlow library onto the DataBricks cluster platform. Then we will be able to import it and to access its functionalities.<br>\nThe following block of code attempts to import TF. If it has not been installed already, an error will be triggered. We will catch such an error and proceed to the installation of TF on DataBricks."],"metadata":{}},{"cell_type":"code","source":["try:\n    import tensorflow as tf\n    print(\"TensorFlow is already installed!\")\nexcept ImportError:\n    print(\"Installing TensorFlow...\")\n    import subprocess\n    subprocess.check_call([\"/databricks/python/bin/pip\", \"install\", \n               \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\"])\n    print(\"TensorFlow has been successfully installed on this cluster\")\n    import tensorflow as tf"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["<h2> 2. 'Shallow' Multi-Layer Perceptron (MLP) </h2>\n\nThe goal of this section is to learn how to build and train a simple MLP architecture. Afterwards, we will apply it to the classification problem of Fisher's iris flower dataset, which we also used in previous lectures.<br>\nGiven that there are only four features available and 150 instances (50 from each of the three flower classes), the use of 'deep' ML solutions may not be the most appropriate choice for this situation. Thus, we will build a traditional <b><i>'shallow'</i></b> MLP with: one input layer of neurons, two hidden layers, and one output layer."],"metadata":{}},{"cell_type":"markdown","source":["<h3> Part 0: Load and preprocess data </h3>\n\nDivide the data in two subsets: 75% of the instances for the purpose of training the MLP classifier, the remaining 25% to test its accuracy on an independent set (once the training will be complete).<br>\nUse the training data to obtain a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">scaling transformation</a> which <b>standardizes features</b>: to have zero mean and unit standard deviation for each. Apply the same scaling to both the training and the test subsets. In this manner, we are protecting ourselves against potential issues that could be caused by features having different units, or very dissimilar ranges of variation, etc.<br>\nBesides, we need to <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html\">binarize</a> the labels which indicate us to which class belongs each instance. For example, if an item belongs to the 3rd class (<i>'iris virginica'</i>), it will be labeled with number 2 in Y. We wish to obtain the <b>binarized equivalent</b> as a <b>vector</b> [0 0 1]."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the random generator seed to compare results\nnp.random.seed(0)\n\n# Load the Fisher's iris flower dataset\niris = datasets.load_iris()\nX = iris.data[:]\nY = iris.target\n\n# Divide into subsets\nX_train, X_test, Y_train, Y_test = train_test_split(<FILL_IN>)\n\n# Apply feature standardization\nscaler = StandardScaler()\nX_train = scaler.fit_transform(<FILL_IN>)\nX_test = scaler.<FILL_IN>\n\n# Binarize the labels\nset_classes = np.unique(Y)\nY_train_bin = label_binarize(<FILL_IN>)\nY_test_bin = label_binarize(<FILL_IN>)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["<h3> Part 1: Define the MLP architecture </h3>\n\nIn this stage we will choose the structure of our MLP neural network.<br>\nFor the <b>input layer, the number of neurons must always be equal to the number of features</b> that we have in our problem (here 4). Similarly, for the <b>output layer there must be one neuron per class</b> (here 3). In this manner, when the network is fed with an input which belongs to the 3rd class -for example-, then the 3rd neuron in the output layer should ideally experience a high activation; whereas the other neurons in the output layer should not activate at all (or very weakly, at most).<br>\nBesides, we need to manually <b>specify the structure of the inner layers</b> of the MLP network (traditionally named as 'hidden'). For now, let's assume that we choose to have two hidden layers with 6 and 5 neurons, respectively."],"metadata":{}},{"cell_type":"code","source":["n_feats = X.shape[1]\nn_class = set_classes.shape[0]\n\n# Network structure\nn_in = n_feats\nn_out = n_class\n\nn_hidden1 = 6\nn_hidden2 = 5"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["The following step consists in creating some structures that will allow us to interact with TensorFlow.<br>\nOur inputs and outputs will be implemented as <a href=\"https://www.tensorflow.org/programmers_guide/reading_data#feeding\">'placeholders'</a>, which are special symbolic variables used by TF to manage data that must be fed on execution time.<br>\nBesides, we need to store the weights and biases that characterize the behaviour of our MLP."],"metadata":{}},{"cell_type":"code","source":["# TensorFlow inputs and outputs\nx = tf.placeholder(\"float\", [None, n_in]) # by setting None, we allow for a dynamic number of instances\ny = tf.placeholder(\"float\", [None, n_out])\n                   \n# Store weights and biases for each layer\nweights = {\n    'wh1': tf.Variable(tf.random_normal([n_in, n_hidden1])),\n    'wh2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n    'wo': tf.Variable(tf.random_normal([n_hidden2, n_out]))\n}\nbiases = {\n    'bh1': tf.Variable(tf.random_normal([n_hidden1])),\n    'bh2': tf.Variable(tf.random_normal([n_hidden2])),\n    'bo': tf.Variable(tf.random_normal([n_out]))\n}"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Practical note: To prevent numerical issues with convergence, it is important to initialize weights and biases with random, non-zero values. In the code above, we are using a normal (i.e. Gaussian) distribution with zero mean and unit standard deviation."],"metadata":{}},{"cell_type":"markdown","source":["<h3> Part 2: MLP function </h3>\n\nIn the theoretical lectures we learnt that the behaviour of a neuron can be undestood as having two stages:\n <ol>\n  <li>the inputs to the neuron are merged by a <b>weighted linear combination, plus an additional bias term</b>;</li>\n  <li>then the <b>activation</b> corresponds to a <b>non-linear</b> function of the result from step 1.</li>\n</ol> \nBut MLPs do not only contain a single neuron. Instead, they have sets of neurons arranged in layers, which all behave in a very similar manner (just with different weights and bias values). This facilitates the calculations, since they can be implemented efficiently as matrix operations - in fact, <b>'tensor'</b> operations: where a <i>'tensor'</i> is a generalization of a matrix to allow for more than two dimensions. Actually, this inspired the name TensorFlow."],"metadata":{}},{"cell_type":"code","source":["# Create our own MLP model with TensorFlow functions\ndef MLP_TF(x_in, weights, biases):\n    # 1st hidden layer, with sigmoidal activation\n    layer1 = tf.add(tf.matmul(x_in, weights['wh1']), biases['bh1']) # linear part: weights_hidden1*x_in+biases_hidden1\n    layer1 = tf.nn.sigmoid(layer1) # non-linear part: sigmoidal activation\n    # 2nd hidden layer, again with sigmodial activation\n    layer2 = tf.add(tf.matmul(<FILL_IN>), <FILL_IN>)\n    layer2 = tf.nn.sigmoid(<FILL_IN>)\n    # Output layer, with linear activation\n    y_out = tf.add(tf.matmul(layer2, weights['wo']), biases['bo'])\n    return y_out"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["As you can see, we decided to <b>omit the non-linearity from the last layer (i.e. the output)</b>. This is a common practise in TF with networks used for classification problems. In this way, the output neurons yield a result which is not directly the predicted likelihood of the input to belong to a certain class; but instead, an estimation of the <b>logarithm of such likelihood</b>. Afterwards, the <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax\">softmax</a> function will transform these activations to choose the most likely class assignment for prediction.<br>\n\nIn the following block of code, the MLP is set up.<br>\nWe must also specify our <b>loss function</b>. Here we choose to employ the <a href=\"https://www.tensorflow.org/api_docs/python/tf/reduce_mean\">mean</a> across all instances of the <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\">cross-entropy</a> between:\n<ol type=\"A\">\n<li><b>predictions (a.k.a. logits)</b>, and</li>\n<li><b>actual classes</b>.</li>\n</ol>"],"metadata":{}},{"cell_type":"code","source":["# Construct model\ny_pred = MLP_TF(x, weights, biases)\n\n# Define our loss function, also called cost \ncosts_all = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y)\ncost = tf.reduce_mean(costs_all)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["<h3> Part 3: Training our MLP </h3>\n\nThe procedure of training the network consists in an <b>iterative adjustment of the values of neurons' weights and biases, aiming to minimize the total misclassification cost</b> that we defined above. To do so, we need to select an optimization algorithm, which is responsible for updating weights and biases.<br>\nTF includes several of these <b>optimizers</b>; but in this example we will use the <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer\">GradientDescentOptimizer</a>, because it is the simplest algorithm available. It estimates the <b>direction of maximal change in the cost function (<i>'gradient'</i>)</b>, and performs the update according to such a direction. Besides, the magnitude of the update depends on a <b>'learning rate'</b>: the higher this learning rate is, the more rapidly we expect to converge to an optimum (i.e. a minimum cost). However, excessive rates may lead to instabilities and non-convergence issues."],"metadata":{}},{"cell_type":"code","source":["# Training and optimizer parameters\nlearn_rate = 0.02\ntrain_iterations = 2500 \noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learn_rate).minimize(cost)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["TF works with <i>'sessions'</i>, meaning that we need to create a session and initialize all our variables before we can actually run the code."],"metadata":{}},{"cell_type":"code","source":["# Initialize TF session and its variables\ninit = tf.initialize_all_variables()\n\nsess = tf.Session() # new TF session\nsess.run(init)\n\ndisplay_step = 100 # just for showing texts indicating progress\n# Training cycle\nfor it in range(train_iterations):\n  # Run optimization (backpropagation) and the calculation of costs (to get loss values)\n  _, c = sess.run([optimizer, cost], feed_dict={x: X_train, y: Y_train_bin})\n  \n  # Display logs per step\n  if it % display_step == 0:\n    print(\"Iteration: %d Cost= %0.6f\" %(it, c))\n  \nprint(\"Optimization finished!\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["<h3> Part 4: Evaluating our MLP </h3>\nLet's now compute the performance of the trained model, when applied on the separate testing set."],"metadata":{}},{"cell_type":"code","source":["# Compare model predictions against 'ground truth' labels\nis_pred_correct = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n\n# Calculate overall accuracy\naccuracy = tf.reduce_mean(tf.cast(is_pred_correct, \"float\"))\naccur_test = accuracy.eval({x: X_test, y: Y_test_bin}, session=sess)\nprint(\"Accuracy test: %2.2f%%\" %(100*accur_test))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["<h3> Advanced work </h3>\nInstead of having a fixed number of neurons in the hidden layers, adapt the code from above to do an <b>automated selection of the MLP structure</b>. You will need not only a training (e.g. 60%) and a testing subset (e.g. 20%) to assess the final performance of the MLP, but also a validation subset (e.g. 20%) to select the most suitable values for the number of neurons in each of the two layer. Assume other important aspects of the network (e.g. activation functions) and about the training procedure (loss function, type of optimization algorithm, learning rate, number of training epochs, etc.) to remain unaltered."],"metadata":{}},{"cell_type":"code","source":["import itertools\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the random generator seed to compare results\nnp.random.seed(0)\n\n# Load the Fisher's iris flower dataset\niris = datasets.load_iris()\nX = iris.data[:]\nY = iris.target\n\n# Divide into subsets\nX_trainval, X_test, Y_trainval, Y_test = train_test_split(<FILL_IN>)\nX_train, X_val, Y_train, Y_val = train_test_split(<FILL_IN>)\n\n# Apply feature standardization\nscaler = StandardScaler()\nX_train = scaler.fit_transform(<FILL_IN>)\nX_valid = scaler.<FILL_IN>\nX_test = scaler.<FILL_IN>\n\n# Binarize the labels\nset_classes = np.unique(Y)\nY_train_bin = label_binarize(<FILL_IN>)\nY_valid_bin = label_binarize(<FILL_IN>)\nY_test_bin = label_binarize(<FILL_IN>)\n\n# Set network structure\nn_feats = X.shape[1]\nn_class = set_classes.shape[0]\nn_in = n_feats\nn_out = n_class\n\n# TensorFlow inputs and outputs\nx = tf.placeholder(<FILL_IN>)\ny = tf.placeholder(<FILL_IN>)\n\n# Create our own MLP model with TensorFlow functions\ndef MLP_TF(x_in, weights, biases):\n  # 1st hidden layer, with sigmoidal activation\n  layer1 = tf.add(tf.matmul(<FILL_IN>), <FILL_IN>)\n  layer1 = tf.nn.sigmoid(<FILL_IN>)\n  # 2nd hidden layer, again with sigmodial activation\n  layer2 = tf.add(tf.matmul(<FILL_IN>), <FILL_IN>)\n  layer2 = tf.nn.sigmoid(<FILL_IN>)\n  # Output layer, with linear activation\n  y_out = tf.add(tf.matmul(<FILL_IN>), <FILL_IN>)\n  return y_out\n\n# Training and optimizer parameters\nlearn_rate = 0.02\ntrain_iterations = 2500\n\n# Manual implementation of a grid search strategy\nn_hidden1_opts = range(4,9) # options to explore: neurons in the 1st hidden layer\nn_hidden2_opts = range(4,9) # options to explore: neurons in the 2nd hidden layer\noptions = list(itertools.product(n_hidden1_opts, n_hidden2_opts)) # all possible combinations of options\naccur_train, accur_valid, accur_test = [], [], []\nprint(\"Please be patient, this code may take some minutes to run!\")\nfor opt in options:\n  # Candidate MLP architecture\n  nh1, nh2 = opt\n  weights = {\n    'wh1': tf.Variable(<FILL_IN>),\n    'wh2': tf.Variable(<FILL_IN>),\n    'wo': tf.Variable(<FILL_IN>)\n  }\n  biases = {\n    'bh1': tf.Variable(<FILL_IN>),\n    'bh2': tf.Variable(<FILL_IN>),\n    'bo': tf.Variable(<FILL_IN>)\n  }\n  \n  # Construct model\n  y_pred = MLP_TF(<FILL_IN>)\n\n  # Define loss function and optimizer\n  costs_all = tf.nn.softmax_cross_entropy_with_logits(logits=<FILL_IN>, labels=<FILL_IN>)\n  cost = tf.reduce_mean(<FILL_IN>)\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate=<FILL_IN>).minimize(<FILL_IN>)\n  \n  # Initialize TF session and its variables\n  init = tf.initialize_all_variables()\n  sess = tf.Session() # new TF session\n  sess.run(init)\n\n  # Training\n  for iter in range(train_iterations):\n  # Run optimization (backpropagation) and cost (to get loss value)\n    sess.run([optimizer, cost], feed_dict={x: <FILL_IN>, y: <FILL_IN>})\n    \n  # Evaluation cycle\n  is_pred_correct = tf.equal(tf.argmax(<FILL_IN>, 1), tf.argmax(<FILL_IN>, 1))\n  accuracy = tf.reduce_mean(tf.cast(<FILL_IN>, \"float\"))\n  \n  accur_train.append( accuracy.eval({x: <FILL_IN>, y: <FILL_IN>}, session=sess) )\n  accur_valid.append( accuracy.eval({x: <FILL_IN>, y: <FILL_IN>}, session=sess) )\n  accur_test.append( accuracy.eval({x: <FILL_IN>, y: <FILL_IN>}, session=sess) )\n  \n# Once all possible combinations have been explored, find which yields the maximal accuracy on the validation set\nprint(\"Optimization finished!\")\n#idx_best = np.argmax(np.array(<FILL_IN>))\nidx_best = np.argmax(np.array(accur_valid))\nprint(\"Valid accuracy @ best valid option: %2.2f%%\" %(100*accur_valid[<FILL_IN>]))\nprint(\"Test accuracy @ best valid option: %2.2f%%\" %(100*accur_test[<FILL_IN>]))\nopt_best = options[<FILL_IN>]\nprint(\"MLP architecture @ best valid option: HiddenLayer#1 = %d, HiddenLayer#2 = %d\" %(<FILL_IN>, <FILL_IN>))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["<h2> 3. Convolutional Neural Network (CNN) </h2>\n\nIn this section we will create a 'deep' CNN to classify small images. The aim is to discern digits present in pictures of handwritten text.<br>\nThus, the problem will be formulated as a classification task with ten possible classes (i.e. digits from 0 to 9)."],"metadata":{}},{"cell_type":"markdown","source":["<h3> Part 0: Dataset </h3>\nWe will use LeCun's <b>MNIST</b> <a href=\"http://yann.lecun.com/exdb/mnist/\">database</a>, a common dataset for benchmarking purposes. It contains 60k training instances, plus other 10k for testing. Each instance is a 28x28 pixel greyscale image matrix, or equivalently a flattened vector of 784 dimensions (components).\n\nThe following block of code will download, read and format the dataset for our use. Note that the training (55k), validation (5k) and testing (10k) subsets are already prepared."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n# The MNIST dataset has 10 classes, representing the digits 0 through 9.\nNUM_CLASSES = 10\n\n# The MNIST images are always 28x28 pixels.\nIMAGE_SIZE = 28\nIMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n\n# Initialize the random generator seed to compare results\nnp.random.seed(0)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Let's now plot a few examples."],"metadata":{}},{"cell_type":"code","source":["examples_first = 0\nexamples_num_cols, examples_num_rows = 8, 8\n\nfig, axes = plt.subplots(nrows=examples_num_rows, ncols=examples_num_cols)\ncount = 0\nfor ax in axes.flat:\n  example = mnist.train.images[examples_first+count]\n  example = np.reshape(example, (<FILL_IN>, <FILL_IN>))\n  ax.imshow(example, cmap='gray')\n  ax.axis('off')\n  count += 1\n\nplt.show()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["<h3> Part 1: CNN architecture and function </h3>\n\nFirstly, we must define the placeholders for our TF inputs and outputs.<br>\nTaking into account that the MNIST dataset contains a large volume of instances, in practise it is not very convenient to process all of them at once. Instead, we will onwards work with smaller subsets, known as <b>batches</b> (<b><i>'lotes'</i></b> in Spanish)."],"metadata":{}},{"cell_type":"code","source":["BATCH_SIZE = 1000\n\nx = tf.placeholder(\"float\", [BATCH_SIZE, IMAGE_PIXELS])\ny = tf.placeholder(\"float\", [BATCH_SIZE, NUM_CLASSES])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["The <b>convolutional</b> behaviour of the CNN resides in exploiting the <b>spatial information</b> present in the images, i.e. relationships among the greyscale value of a certain pixel and those of its local neighbours.<br>\nFor this reason, we must reshape each individual data instance a into matrix image of size 28x28 pixels. The following code does that task in an efficient manner, processing the whole batch at once. By specifying the first dimension as -1, we keep the batch size without change. By setting the last dimension to 1, we mean that there exists a single color channel (greyscale, in our case). Note that coloured images (e.g. RGB) will in general need several channels, typically 3.<br>\nThus, in the end we will obtain a <b>4D-tensor</b> of size [1000 (batch size), 28, 28, 1]."],"metadata":{}},{"cell_type":"code","source":["# Convert the batch of images into a 4D-tensor\nx_image = tf.reshape(x, [-1,IMAGE_SIZE,IMAGE_SIZE,1])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["The response of the CNN architecture (in its different layers) will be characterized by a series of <b>variables</b>, equivalent or analogous to the weights and biases in the MLP function from the past section. For convenience, we will now define a couple of auxiliary functions to facilitate us the creation of such variables.<br>\nIn most circumstances, we would initialize them with random values, e.g. following a <b>normal</b> distribution. However, for the specific case of biases that enter a neuron with ReLU activation, it is more practical to start with small positive values. The <b>constant</b> generator below will deal with that task."],"metadata":{}},{"cell_type":"code","source":["def init_random_normal_variable(shape, std_dev):\n  initial = tf.random_normal(shape, stddev=std_dev)\n  return tf.Variable(initial)\n\ndef init_constant_variable(shape, const_val):\n  initial = tf.constant(const_val, shape=shape)\n  return tf.Variable(initial)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["As we saw in the theoretical lectures, the basic operations in CNNs are <b>convolution</b> and <b>pooling</b>, which will be repeated through the convolutional layers. Therefore, it is also practical to define these schemes as function blocks.<br>\nOur choice below is to perform a 'classical' 2D convolution, but TF has other <a href=https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/convolution>convolution possibilities</a> available. For instance, 3D convolution may be of high relevance when dealing with video or 3D images (e.g. medical volumetric scans). As for the pooling operation, 'max' is the most common choice in various applications, but again TF has implemented <a href=https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/pooling>other pooling options</a>, e.g. average."],"metadata":{}},{"cell_type":"code","source":["def conv2d_fcn(x_img, filt_weights):\n  return tf.nn.conv2d(x_img, filt_weights, strides=[1, 1, 1, 1], padding='SAME')\n  \ndef max_pool_fcn(x_img, pool_size):\n  return tf.nn.max_pool(x_img, ksize=[1, pool_size, pool_size, 1], strides=[1, pool_size, pool_size, 1], padding='SAME')"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Note that the <b>stride</b> establishes how much the convolutional/pooling block is moved in each dimension after one iteration.<br>\n<b>Padding</b> specifies how the edges of the image are treated when doing the operations. For TF, <i>'SAME'</i> means zero padding (i.e. values beyond the edges are assumed to equal zero); whereas <i>'VALID'</i> indicates that we do not operate beyond the edges."],"metadata":{}},{"cell_type":"markdown","source":["Now we have defined the main building pieces for a CNN layer. Let's proceed to create one."],"metadata":{}},{"cell_type":"code","source":["CONV1_SIZE, CONV1_NUM_FEATS = 5, 32\nstdev_w1, const_b1 = 0.2, 0.1 # for example\n# Create a bank of 32 convolutional filters of size 5x5 pixels\nconv1_weights = init_random_normal_variable([CONV1_SIZE, CONV1_SIZE, 1, CONV1_NUM_FEATS], stdev_w1)\nconv1_bias = init_constant_variable([CONV1_NUM_FEATS], const_b1)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Besides, we must choose an <a href=https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/activation_functions_> activation function</a>. In this case, let's use the rectified linear unit (ReLU), since it tends to provide quick convergence properties in various types of scenarios. Nonetheless, bear in mind that other activations may be more suitable for different problems."],"metadata":{}},{"cell_type":"code","source":["conv1_in = x_image\nconv1_activ = tf.add(conv2d_fcn(conv1_in, conv1_weights), conv1_bias) # convolution 2D and biases added\nconv1_activ = tf.nn.relu(conv1_activ) # ReLU activation"],"metadata":{"collapsed":true},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Next, let's incorporate the pooling stage."],"metadata":{}},{"cell_type":"code","source":["POOL1_SIZE = 2\npool1 = max_pool_fcn(conv1_activ, POOL1_SIZE)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["<b>Task</b>: Create yourself a similar second convolutional layer with:\n* Convolutional blocks of size 3x3, 48 features and the appropriate number of channels.\n* Softplus activation (which implies random initialization of biases).\n* Average pooling with size 2x2."],"metadata":{}},{"cell_type":"code","source":["CONV2_SIZE, CONV2_NUM_FEATS = <FILL_IN>, <FILL_IN>\nstdev_w2, stdev_b2 = <FILL_IN>, <FILL_IN>\n\nconv2_weights = <FILL_IN>\nconv2_bias = <FILL_IN>\n\nconv2_in = <FILL_IN>\nconv2_activ = tf.add(conv2d_fcn(<FILL_IN>, <FILL_IN>), <FILL_IN>) # convolution 2D and biases added\nconv2_activ = tf.nn.<FILL_IN> # softplus activation\n\ndef avg_pool_fcn(x_img, pool_size):\n  return tf.nn.<FILL_IN>\n\nPOOL2_SIZE = <FILL_IN>\npool2 = <FILL_IN>"],"metadata":{"collapsed":true},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["At last, it is a common practise to add a final fully connected layer (as in a 'classical' MLP), with softmax activation."],"metadata":{}},{"cell_type":"code","source":["# Flatten the input tensor (i.e. the output of the 2nd convolutional layer) into a vector\nfull_in = pool2\nfull_in_flat = tf.reshape(full_in, [BATCH_SIZE, -1])\nNUM_FULL_INPUTS = full_in_flat.get_shape()[1].value\n\n# Hidden layer\nNUM_HIDDEN_NEURONS = 128\nstdev_wfih, stdev_bfih = 0.2, 0.2 # for example\nfull_in_hid_weights = init_random_normal_variable([NUM_FULL_INPUTS, NUM_HIDDEN_NEURONS], stdev_wfih)\nfull_in_hid_bias = init_random_normal_variable([NUM_HIDDEN_NEURONS], stdev_bfih)\n\nfull_hid_activ = tf.add(tf.matmul(full_in_flat, full_in_hid_weights), full_in_hid_bias)\nfull_hid_activ = tf.nn.sigmoid(full_hid_activ) # sigmoid activation\n\n# Output layer\nNUM_FULL_OUTPUTS = NUM_CLASSES\nstdev_wfho, stdev_bfho = 0.2, 0.2 # for example\nfull_hid_out_weights = init_random_normal_variable([NUM_HIDDEN_NEURONS, NUM_FULL_OUTPUTS], stdev_wfho)\nfull_hid_out_bias = init_random_normal_variable([NUM_FULL_OUTPUTS], stdev_bfho)\n\nfull_out_activ = tf.add(tf.matmul(full_hid_activ, full_hid_out_weights), full_hid_out_bias) # linear activation\n\n# Final, global output\ny_cnn = full_out_activ # remember that softmax will be applied later on"],"metadata":{"collapsed":true},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["That completes the structure of our CNN. Next, the overall <b>cost function</b> will be as follows:"],"metadata":{}},{"cell_type":"code","source":["# Define our loss function \ncosts_all_cnn = tf.nn.softmax_cross_entropy_with_logits(logits=y_cnn, labels=y)\ncost_cnn = tf.reduce_mean(costs_all_cnn)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["<h3> Part 2: Training our CNN </h3>\n\nAs for the previous case of the MLP architecture, the procedure of training the CNN aims at <b>minimizing the misclassification cost</b> (cross-entropy, here) over the training subset of data. It consists in an <b>iterative adjustment</b> of the values of the constituent pieces of the network: the coefficients of the convolutional filters, the biases of the activation functions, the weights and biases of the fully connected part, etc.<br>\n\nWe must again select an <b>optimization algorithm</b> which updates these elements. We could use the same GradientDescentOptimizer that we suggested for the MLP, but let's now explore another computationally efficient method called <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/train/optimizers#AdamOptimizer\">AdamOptimizer</a> (if interested, you can find a detailed theoretical description of the algorithm <a href=\"https://arxiv.org/abs/1412.6980\">here</a>).<br>\n\nBesides, there is a <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/train/optimizers\">list</a> of optimizers available in TF."],"metadata":{}},{"cell_type":"markdown","source":["Adam has a series of <b>hyper-parameters</b> which define its behaviour as optimizer. In broad terms, it requires us to set a learning rate, as well as two exponential forgetting factors (beta1, beta2). Let's now introduce the default values for these three hyper-parameters."],"metadata":{}},{"cell_type":"code","source":["adam_learn_rate = 0.001\nadam_beta1, adam_beta2 = 0.9, 0.999\n# Creation of the optimizer\nadam_optimizer = tf.train.AdamOptimizer(learning_rate=adam_learn_rate, beta1=adam_beta1, beta2=adam_beta2).minimize(cost_cnn)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["Let's also add some code to calculate the total results in terms of classification performance."],"metadata":{}},{"cell_type":"code","source":["# Compute a vector of classification error indicators, comparing true labels with CNN predictions\nis_pred_correct_cnn = tf.equal(tf.argmax(<FILL_IN>, 1), tf.argmax(<FILL_IN>, 1))\n\n# Estimate training accuracy as the rate of errors in the given dataset\naccuracy_cnn = tf.reduce_mean(tf.cast(<FILL_IN>, \"float\"))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["We are ready to run the TF session. We will see how to apply batch gradient updates:"],"metadata":{}},{"cell_type":"code","source":["NUM_IMAGES_TRAIN = mnist.train.images.shape[0] # 55 thousand\nNUM_BATCHES_TRAIN = NUM_IMAGES_TRAIN/BATCH_SIZE\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nadam_train_iterations = 50\ndisplay_step = 5\nprint(\"Please be patient, this code will need a very long time to run (approx. 60 minutes in DataBricks)!\")\nfor i in range(adam_train_iterations):\n  for b in range(NUM_BATCHES_TRAIN):\n    batch_train = mnist.train.next_batch(BATCH_SIZE) # get the following batch of training images\n    # Perform batch training\n    sess.run(adam_optimizer, feed_dict = {x: batch_train[0], y: batch_train[1]})\n    \n  if i % display_step == 0:\n    accur_train = sess.run(accuracy_cnn, feed_dict = {x: batch_train[0], y: batch_train[1]})\n    print(\"Iteration %d, Training accuracy = %2.2f%%\" %(i, 100*accur_train))\n    \nprint(\"Optimization finished!\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["<h3> Part 3: CNN evaluation </h3>\nFinally, we should check how accurately the trained CNN performs when applied to the testing set."],"metadata":{}},{"cell_type":"code","source":["NUM_IMAGES_TEST = mnist.test.images.shape[0] # 10 thousand\nNUM_BATCHES_TEST = NUM_IMAGES_TEST/BATCH_SIZE\n\ntrue_labels, pred_labels = np.empty([0, 1]), np.empty([0, 1])\nfor b in range(NUM_BATCHES_TEST):\n  batch_init, batch_end = BATCH_SIZE*b, BATCH_SIZE*(b+1)\n  # Get the next batch of test images\n  x_batch_test = mnist.test.images[batch_init:batch_end]\n  y_batch_test = mnist.test.labels[batch_init:batch_end]\n  # Perform evaluation on this test batch\n  batch_pred = sess.run(tf.argmax(y_cnn, 1), feed_dict = {x: x_batch_test})\n  batch_true = sess.run(tf.argmax(y, 1), feed_dict = {y: y_batch_test})\n  # Store the results together with those from previous test batches\n  pred_labels = np.vstack((pred_labels, batch_pred.reshape([-1, 1])))\n  true_labels = np.vstack((true_labels, batch_true.reshape([-1, 1])))\n\naccur_test = np.mean(np.equal(true_labels, pred_labels))\nprint(\"Test accuracy = %2.2f%%\" %(100*accur_test))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["<b>Task 1</b>: Check the dimensions of each of the tensors involved in the CNN. To do so, use TF's get_shape() function. Explain why the dimensions are those."],"metadata":{}},{"cell_type":"code","source":["<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["<b>Task 2</b>: Check the CNN decisions for the wrongly classified digits in the testing set. Find out which examples had a failure. Plot them, indicating both the true digit and the predicted label."],"metadata":{}},{"cell_type":"code","source":["idx_wrong_test = np.nonzero(np.not_equal(<FILL_IN>, <FILL_IN>))[0]\nnum_wrong_test = <FILL_IN>\nprint(\"Number of wrongly classified test instances: %d\" %(num_wrong_test))\n\nnum_cols = 10 # to arrange the plots\nnum_rows = np.int8(np.ceil(np.true_divide(num_wrong_test, num_cols)))\n\nfig, axes = plt.subplots(nrows=num_rows, ncols=num_cols)\ncount = 0\nfor ax in axes.flat:\n  if count >= num_wrong_test:\n    ax.axis('off')\n    continue # no more wrong items to plot\n  \n  idx = idx_wrong_test[count]\n  # Get the image\n  example = mnist.test.images[<FILL_IN>]\n  example = np.reshape(<FILL_IN>, <FILL_IN>)\n  ax.imshow(example, cmap='gray')\n  ax.axis('off')\n  \n  # Get the labels\n  true_label = <FILL_IN>\n  pred_label = <FILL_IN>\n  example_title = \"True: %d; Pred: %d\" %(true_label, pred_label)\n  ax.title.set_text(example_title)\n  ax.title.set_fontsize(5)\n  \n  count += 1\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1) # this line is just for a more comfortable display of images and titles\nplt.show()\ndisplay(fig)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["<h3> Advanced work </h3>\n<b>Task 1</b>: Plot the weights of the trained convolutional filters for layers 1 and 2. This will give you a visual hint why the CNN can be interpreted as performing some kind of automated feature extraction."],"metadata":{}},{"cell_type":"code","source":["# Convolutional layer 1\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["# Convolutional layer 2\n<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["<b>Task 2</b>: Create your own CNN architecture. Be creative: add or remove layers, change the size and/or the number of convolutional filters, the type and size of pooling, the non-linear activation functions, etc. Try modifying the architecture of the fully connected part of the network. Explore different optimizer algorithms."],"metadata":{}},{"cell_type":"code","source":["<FILL_IN>"],"metadata":{},"outputs":[],"execution_count":67}],"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2.0},"version":"2.7.12","nbconvert_exporter":"python","file_extension":".py"},"name":"A4_MLP","notebookId":3299202557913368},"nbformat":4,"nbformat_minor":0}
