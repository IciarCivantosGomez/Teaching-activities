{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis.\n",
    "\n",
    "Author: JesÃºs Cid Sueiro\n",
    "Date: 2016/04/03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore some tools for text analysis in python. To do so, first we will import the requested python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from wikitools import wiki\n",
    "from wikitools import category\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# Garbage\n",
    "#import numpy as np\n",
    "#import nltk.stem\n",
    "#from gensim import corpora, models, matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus acquisition.\n",
    "\n",
    "There are many available text collections to test topic modelling algorithm. In particular, the NLTK library has many examples. You can explore them using the `nltk.download()` tool.\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "    Mycorpus = nltk.corpus.gutenberg\n",
    "    text_name = Mycorpus.fileids()[0]\n",
    "    raw = Mycorpus.raw(text_name)\n",
    "    Words = Mycorpus.words(text_name)\n",
    "\n",
    "In this notebook, we will explore and analyize collections of Wikipedia articles from a given category. Thus, we will not use the availabel corpora at nltk, but the `wikitools` library, that makes easy the capture of content from wikimedia sites.\n",
    "\n",
    "We will explore the category ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudoscience\n"
     ]
    }
   ],
   "source": [
    "site = wiki.Wiki(\"https://en.wikipedia.org/w/api.php\")\n",
    "cat = \"Pseudoscience\"\n",
    "print cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... though you can try with any other categories. Take into account that the behavior of topic modelling algorithms may depend on the amount of documents available for the analysis.\n",
    "\n",
    "We start downloading the text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading category data. This may take a while...\n",
      "Loading article ... 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 Loaded 337 articles from category Pseudoscience\n"
     ]
    }
   ],
   "source": [
    "# Loading category data. This may take a while\n",
    "print \"Loading category data. This may take a while...\"\n",
    "cat_data = category.Category(site, cat)\n",
    "\n",
    "corpus_titles = []\n",
    "corpus_text = []\n",
    "n = 0\n",
    "print \"Loading article ...\",\n",
    "for page in cat_data.getAllMembersGen():\n",
    "    n += 1\n",
    "    print n,\n",
    "    corpus_titles.append(page.title)\n",
    "    corpus_text.append(page.getWikiText())\n",
    "\n",
    "n_art = len(corpus_titles)\n",
    "print \"\\nLoaded \" + str(n_art) + \" articles from category \" + cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have stored the whole text collection in two lists:\n",
    "\n",
    "* `corpus_titles`, which contains the titles of the selected articles\n",
    "* `corpus_text`, with the text content of the selected wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus Processing\n",
    "\n",
    "Topic modelling algorithms process vectorized data. In order to apply them, we need to transforms the raw text input data into vector representation. To do so, we need to process the text data in order to remove irrelevant information and preserve as much relevant information as possible to capture the semantic content in the document collection.\n",
    "\n",
    "Thus, we will proceed with the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization\n",
    "3. Cleaning\n",
    "4. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization\n",
    "\n",
    "In order to use the `word_tokenize` method from nltk, you might need to get the appropriate libraries using \n",
    "\n",
    "    nltk.download()\n",
    "    # select option \"d) Download\", and identifier \"punkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# select option \"d) Download\", and identifier \"punkt\"\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing article 337 out of 337 \n",
      " The corpus has been tokenized. Let's check some portion of the first article:\n",
      "[u'{', u'{', u'npov|date=March', u'2016', u'}', u'}', u'{', u'{', u'synthesis|date=March', u'2016', u'}', u'}', u\"'\", u\"''\", u'Cryptozoology', u\"''\", u\"'\", u'is', u'a', u'[', u'[', u'pseudoscience', u']', u']', u'involving', u'the', u'search', u'for', u'creatures', u'whose']\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = []\n",
    "n = 0\n",
    "\n",
    "for art in corpus_text: \n",
    "    n += 1\n",
    "    print \"\\rTokenizing article {0} out of {1}\".format(n, n_art),\n",
    "\n",
    "    # This is to remove strange characters\n",
    "    art = art.decode('utf-8')    \n",
    "\n",
    "    # Tokenize each text entry\n",
    "    tokens = word_tokenize(art)\n",
    "\n",
    "    corpus_tokens.append(tokens)\n",
    "    \n",
    "print \"\\n The corpus has been tokenized. Let's check some portion of the first article:\"\n",
    "print corpus_tokens[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Homogeneization\n",
    "\n",
    "By looking at the tokenized corpus you may verify that there are many tokens that correpond to punktuation signs and other symbols that are not relevant to analyze the semantic content. They can be removed using the stemming tool from `nltk`.\n",
    "\n",
    "The homogeneization process will consist of:\n",
    "\n",
    "1. Removing capitalization: capital alphabetic characters will be transformed to their corresponding lowercase characters.\n",
    "2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "3. Stemming: removing word terminations to preserve the rood of the words and ignore grammatical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming article 337 out of 337 \n",
      "Let's check the first tokens after stemming:\n",
      "[u'2016', u'2016', u'cryptozoolog', u'is', u'a', u'pseudosci', u'involv', u'the', u'search', u'for', u'creatur', u'whose', u'exist', u'has', u'not', u'been', u'proven', u'due', u'to', u'lack', u'of', u'evid', u'the', u'anim', u'cryptozoologist', u'studi', u'are', u'refer', u'to', u'as']\n"
     ]
    }
   ],
   "source": [
    "# Select stemmer.\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "corpus_stemmed = []\n",
    "n = 0\n",
    "for tokens in corpus_tokens:\n",
    "    n +=1\n",
    "    print \"\\rStemming article {0} out of {1}\".format(n, n_art),\n",
    "\n",
    "    # Set to lowercas, remove non alfanumeric tokens and stem.\n",
    "    clean_tokens = [s.stem(token.lower()) for token in tokens if token.isalnum()]\n",
    "    corpus_stemmed.append(clean_tokens)\n",
    "\n",
    "print \"\\nLet's check the first tokens after stemming:\"\n",
    "print corpus_stemmed[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cleaning\n",
    "\n",
    "The third stepd consists on removing those words that are very common in language and do not carry out usefull semantic content (articles, pronouns, etc).\n",
    "\n",
    "Once again, we might need to load the stopword files using the download tools from `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# You can comment this if the package is already available.\n",
    "# select option \"d) Download\", and identifier \"stopwords\"\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords from article 337 out of 337 \n",
      " Let's check tokens after cleaning:\n",
      "[u'2016', u'2016', u'cryptozoolog', u'pseudosci', u'involv', u'search', u'creatur', u'whose', u'exist', u'proven', u'due', u'lack', u'evid', u'anim', u'cryptozoologist', u'studi', u'refer', u'cryptid', u'cryptozoologist', u'includ', u'live', u'exampl', u'creatur', u'otherwis', u'consid', u'extinct', u'live', u'dinosaur', u'cryptozoolog', u'dinosaur']\n"
     ]
    }
   ],
   "source": [
    "corpus_clean = []\n",
    "\n",
    "n = 0\n",
    "for tokens in corpus_stemmed:\n",
    "    n += 1\n",
    "    print \"\\rRemoving stopwords from article {0} out of {1}\".format(n, n_art),\n",
    "\n",
    "    clean_tokens = [token for token in tokens if token not in stopwords.words('english')]    \n",
    "    corpus_clean.append(clean_tokens)\n",
    "    # corpus_clean.append(' '.join(clean_tokens)) (not used)\n",
    "    \n",
    "print \"\\n Let's check tokens after cleaning:\"\n",
    "print corpus_clean[0][0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vectorization\n",
    "\n",
    "Up to this point, we have transformed the raw text collection of articles in a list of articles, where each article is a collection of the word roots that are most relevant for semantic analysis. Now, we need to convert these text data into a numerical representation. To do so, we will start using the tools provided by the `gensim` library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary of tokens\n",
    "D = corpora.Dictionary(corpus_clean)\n",
    "\n",
    "# Transform token lists into sparse vectors on the D-space\n",
    "bow = [D.doc2bow(doc) for doc in corpus_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is good to make sure to understand what has happened. In `corpus_clean` we had a list of token list. With it, we have constructed a Dictionary, `D`, which assign an integer identifier to each token in the corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tokens in the dictionary: \n",
      "0: lack\n",
      "1: focus\n",
      "2: slick\n",
      "3: four\n",
      "4: stumbl\n",
      "5: francesco\n",
      "6: follow\n",
      "7: whose\n",
      "8: privat\n",
      "9: tv\n"
     ]
    }
   ],
   "source": [
    "print \"First tokens in the dictionary: \"\n",
    "for n in range(10):\n",
    "    print str(n) + \": \" + D[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we have transformed each article (in `corpus_clean`) into a sparse vector. These sparse vectors will be the inputs to the topic modeling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original article (after cleaning): \n",
      "[u'2016', u'2016', u'cryptozoolog', u'pseudosci', u'involv', u'search', u'creatur', u'whose', u'exist', u'proven', u'due', u'lack', u'evid', u'anim', u'cryptozoologist', u'studi', u'refer', u'cryptid', u'cryptozoologist', u'includ', u'live', u'exampl', u'creatur', u'otherwis', u'consid', u'extinct', u'live', u'dinosaur', u'cryptozoolog', u'dinosaur']\n",
      "Sparse vector representation (first 30 components):\n",
      "[(0, 3), (1, 2), (2, 1), (3, 1), (4, 1), (5, 5), (6, 2), (7, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 2), (14, 2), (15, 1), (16, 1), (17, 1), (18, 5), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2)]\n",
      "The first component, (0, 3), states that in article number 0, token 0 (lack) appears 3 times\n"
     ]
    }
   ],
   "source": [
    "print \"Original article (after cleaning): \"\n",
    "print corpus_clean[0][0:30]\n",
    "print \"Sparse vector representation (first 30 components):\"\n",
    "print bow[0][0:30]\n",
    "print \"The first component, \" +  str(bow[0][0]) + \", states that in article number 0,\",\n",
    "print \"token 0 (\" + D[0] + \") appears \" + str(bow[0][0][1]) + \" times\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Semantic Analyisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Garbage.\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk.stem\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# import numpy as np\n",
    "#Â import urllib\n",
    "# import codecs\n",
    "# import wikipedia\n",
    "# import pickle\n",
    "# from scipy.sparse import vstack\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "# from sklearn import svm\n",
    "# import sys\n",
    "\n",
    "\"\"\"\n",
    "Funcion para detectar si todos los caracteres de una determinada palabra\n",
    "estan en formato ascii. Extraida de:\n",
    "\n",
    "http://stackoverflow.com/questions/196345/how-to-check-if-a-string-in-python-is-in-ascii\n",
    "\"\"\"\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "\"\"\"\n",
    "Funcion para imprimir los topics de un modelo LDA. Extraida de:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\"\"\"\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    print \"\\n\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "#Clase para seleccionar Mozilla como cliente para realizar peticiones HTTP.\n",
    "class AppURLopener(urllib.FancyURLopener):\n",
    "    version = \"Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11\"\n",
    "\n",
    "\"\"\"\n",
    "Funcion para descargar automaticamente todos los articulos de una determinada\n",
    "categoria desde Wikipedia. Referencias:\n",
    "\n",
    "http://stackoverflow.com/questions/7958191/download-articles-from-wikipedia-using-special-export\n",
    "https://wikipedia.readthedocs.org/en/latest/code.html#api\n",
    "\"\"\"\n",
    "   \n",
    "def descargaArticulos(nombreCategoria):\n",
    "    #Se crea un archivo donde volcar el XML.\n",
    "    f =  codecs.open('workfile2.xml', 'w',\"utf-8\" )\n",
    "    \n",
    "    #Se selecciona Mozilla como cliente para realizar la peticion.\n",
    "    urllib._urlopener = AppURLopener()\n",
    "    #Se forma la peticion HTTP para buscar los articulos de la categoria\n",
    "    query = \"http://en.wikipedia.org/w/index.php?title=Special:Export&action=submit\"\n",
    "    data = { 'catname':nombreCategoria,'addcat':'', 'wpDownload':1 }\n",
    "    data = urllib.urlencode(data)\n",
    "    # Se manda la peticiÃ³n y se almacena el resultado.\n",
    "    f = urllib.urlopen(query, data)\n",
    "    s = f.read()\n",
    "    \n",
    "    #Se parsea el XML descargado.\n",
    "    \"\"\"\n",
    "    Los titulos de los articulos aparecen a partir de un string que pone\n",
    "    name=\"pages\". Se divide el documento en trozos cada vez que aparece\n",
    "    ese string. Los titulos de los articulos acaban cuando aparece el string\n",
    "    </textarea>. Se divide nuevamente el texto cuando aparece ese string, de\n",
    "    forma que se obtiene el nombre de los articulos. Finalmente, para obtener\n",
    "    individualmente los nombres, se divide el texto por el string \\n\n",
    "    \"\"\"\n",
    "    div1 = s.split('name=\"pages\">')\n",
    "    div2 = div1[1].split(\"</textarea>\")[0]\n",
    "    titulos = div2.split(\"\\n\")\n",
    "\n",
    "    articulos = []\n",
    "    # Se descargan los articulos mediante la API de Wikipedia y se guardan.\n",
    "    for titulo in titulos:\n",
    "        if (titulo.startswith(\"Category:\") == 0):\n",
    "            print titulo\n",
    "            try:\n",
    "                articulos.append(wikipedia.page(titulo).content)\n",
    "            #Se ponen excepciones para evitar fallos en la descarga.\n",
    "            except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):\n",
    "                pass\n",
    "\n",
    "    return articulos\n",
    "\n",
    "\"\"\"\n",
    "PROGRAMA PRINCIPAL\n",
    "\"\"\"\n",
    "\n",
    "#Se pregunta al usuario si quiere descargar categorias de forma automatica o si quiere utilizar\n",
    "#las predeterminadas (American Social Sciences Writers y Fictional Families)\n",
    "desAuto = raw_input(\"Introduce S para descargar las categorias o N para usar las predeterminadas: \")\n",
    "while (desAuto != \"S\" and desAuto != \"N\"):\n",
    "    desAuto = raw_input(\"\\nIntroduce una opcion valida por favor [S\\N]:  \")\n",
    "if(desAuto == \"S\"): #Se solicita descarga de categorias.\n",
    "    #Se pide al usuario el nombre de las categorias y se descargan.\n",
    "    cat1 = raw_input('Introduce el nombre de la primera categoria: ')\n",
    "    cat2 = raw_input('\\nIntroduce el nombre de la segunda categoria: ')\n",
    "    print \"Descargando categoria: \" + cat1 + \"\\n\"\n",
    "    print \"Tratando de descargar los siguientes articulos: \\n\"\n",
    "    articulos1 = descargaArticulos(cat1)\n",
    "    print \"Descargando categoria: \" + cat2 + \"\\n\"\n",
    "    print \"Tratando de descargar los siguientes articulos: \\n\"\n",
    "    articulos2 = descargaArticulos(cat2)\n",
    "    print \"\\nArticulos descargados de la categoria 1: \" + str(len(articulos1))\n",
    "    print \"\\nArticulos descargados de la categoria 2: \" + str(len(articulos2))  \n",
    "else: #Se usan las categorias predeterminadas.\n",
    "    print(\"\\nUTILIZANDO CATEGORIAS PREDETERMINADAS\") \n",
    "    sys.stdout.flush()\n",
    "    f = open('articulosAmericanSocial.txt', 'rb')\n",
    "    articulos1 = pickle.load(f)\n",
    "    f = open('articulosFictionalFamilies.txt', 'rb')\n",
    "    articulos2 = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "PREPROCESADO DE LOS ARTICULOS\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nREALIZANDO EL PREPROCESADO DE LOS ARTICULOS\")\n",
    "sys.stdout.flush()\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "#Procesado de los articulos de la primera categoria.\n",
    "textos1 = list()\n",
    "\n",
    "for articulo in articulos1:\n",
    "    #Se divide el articulo en tokens.\n",
    "    tokens = word_tokenize(articulo)\n",
    "    #Se guarda la raiz de aquellos tokens que son alfanumericos.\n",
    "    tokens = [s.stem(token.lower()) for token in tokens if token.isalnum()]\n",
    "    #Se eliminan los tokens que estan en las stopwords.\n",
    "    tokens = [token for token in tokens if token not in eng_stopwords]\n",
    "    #Se eliminan los tokens que no son ascii o que son \"http\" o \"ref\".\n",
    "    tokens = [token for token in tokens if (is_ascii(token) and token!=\"http\" and token!=\"ref\")]\n",
    "    textos1.append(' '.join(tokens))\n",
    "\n",
    "#Procesado de los articulos de la segunda categoria. Identico que para la primera.\n",
    "textos2 = list()\n",
    "\n",
    "for articulo in articulos2:\n",
    "    tokens = word_tokenize(articulo)\n",
    "    tokens = [s.stem(token.lower()) for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if (token not in eng_stopwords  and token!=\"american\" and token!=\"social\" and token!=\"sciences\" and token!=\"writers\" and token!=\"fictional\" and token!=\"families\")]\n",
    "    tokens = [token for token in tokens if (is_ascii(token) and token!=\"http\" and token!=\"ref\")]\n",
    "    textos2.append(' '.join(tokens))\n",
    "\n",
    "\"\"\"\n",
    "Se convierten los textos a una matriz en la que cada palabra es una columna\n",
    "cuyo valor es el numero de veces que aparece dicha palabra en cada documento\n",
    "\"\"\"\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(textos1+textos2)\n",
    "\n",
    "#Se dividen los textos en conjunto de entrenamiento y test.\n",
    "#Porcentaje de textos del total que se van a usar para entrenamiento\n",
    "division = 0.7\n",
    "\n",
    "#Se dividen los textos en train y en test\n",
    "tftrain1 = tf[0:round(len(textos1)*division)]\n",
    "tftrain2 = tf[len(textos1):(len(textos1)+round(len(textos2)*division))]\n",
    "tftrain = vstack((tftrain1,tftrain2))\n",
    "tftest1 = tf[round(len(textos1)*division):len(textos1)]\n",
    "tftest2 = tf[(len(textos1)+round(len(textos2)*division)):len(textos1+textos2)]\n",
    "tftest = vstack((tftest1,tftest2))\n",
    "\n",
    "\"\"\"\n",
    "ALGORITMO LDA: Extraccion de topics.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nREALIZANDO EL LDA Y LA CLASIFICACION\")\n",
    "sys.stdout.flush()\n",
    "#Se define el algoritmo LDA, el cual se entrena con el conjunto de entrenamiento.\n",
    "ldaBagging = LatentDirichletAllocation(n_topics=21, max_iter=3, learning_method='online', learning_offset=50., random_state=0)\n",
    "ldaBagging.fit(tftrain)\n",
    "#Se extraen los datos de entrenamiento y de test, convertidos a valores numericos.\n",
    "X_trainBagging = ldaBagging.transform(tftrain)\n",
    "X_testBagging = ldaBagging.transform(tftest)\n",
    "\n",
    "#Mismo proceso para los demas clasificadores:\n",
    "ldaExtraTrees = LatentDirichletAllocation(n_topics=52, max_iter=9, learning_method='online', learning_offset=50., random_state=0)\n",
    "ldaExtraTrees.fit(tftrain)\n",
    "ldaExtraTrees.\n",
    "X_trainExtraTrees = ldaExtraTrees.transform(tftrain)\n",
    "X_testExtraTrees = ldaExtraTrees.transform(tftest)\n",
    "\n",
    "ldaSVC = LatentDirichletAllocation(n_topics=6, max_iter=8, learning_method='online', learning_offset=50., random_state=0)\n",
    "ldaSVC.fit(tftrain)\n",
    "X_trainSVC = ldaSVC.transform(tftrain)\n",
    "X_testSVC = ldaSVC.transform(tftest)\n",
    "\n",
    "#Se genera el Y_train, asignando 1 a los articulos de la categoria 1 y -1 a los de la categoria 2.\n",
    "Y_train1 = [1]*tftrain1.shape[0]\n",
    "Y_train2 = [-1]*tftrain2.shape[0]\n",
    "Y_test1 = [1]*tftest1.shape[0]\n",
    "Y_test2 = [-1]*tftest2.shape[0]\n",
    "\n",
    "Y_train = np.concatenate((Y_train1,Y_train2),axis=0)\n",
    "Y_test = np.concatenate((Y_test1,Y_test2),axis=0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CLASIFICACION DEL CONJUNTO DE TEST\n",
    "\"\"\"\n",
    "\n",
    "#Clasificador ExtraTrees:\n",
    "#Normalizado de X_train y X_test.\n",
    "scaler = preprocessing.StandardScaler().fit(X_trainExtraTrees)\n",
    "X_trainExtraTrees = scaler.transform(X_trainExtraTrees)\n",
    "X_testExtraTrees = scaler.transform(X_testExtraTrees)\n",
    "extraTrees = ExtraTreesClassifier(n_estimators=600,max_features='log2',random_state=8)\n",
    "extraTrees.fit(X_trainExtraTrees,Y_train.ravel())\n",
    "salida1 = extraTrees.predict(X_testExtraTrees)\n",
    "acierto1 = extraTrees.score(X_testExtraTrees,Y_test)\n",
    "\n",
    "#Clasificador Bagging:\n",
    "#Normalizado de X_train y X_test.\n",
    "scaler = preprocessing.StandardScaler().fit(X_trainBagging)\n",
    "X_trainBagging = scaler.transform(X_trainBagging)\n",
    "X_testBagging = scaler.transform(X_testBagging)\n",
    "bagging = BaggingClassifier(n_estimators=600,random_state=8)\n",
    "bagging.fit(X_trainBagging,Y_train.ravel())\n",
    "salida2 = bagging.predict(X_testBagging)\n",
    "acierto2 = bagging.score(X_testBagging,Y_test)\n",
    "\n",
    "#Clasificador SVC:\n",
    "scaler = preprocessing.StandardScaler().fit(X_trainSVC)\n",
    "X_trainSVC = scaler.transform(X_trainSVC)\n",
    "X_testSVC = scaler.transform(X_testSVC)\n",
    "clasSVC = svm.SVC(C=41)\n",
    "clasSVC.fit(X_trainSVC,Y_train.ravel())\n",
    "salida3 = clasSVC.predict(X_testSVC)  \n",
    "acierto3 = clasSVC.score(X_testSVC,Y_test)\n",
    "\n",
    "#Moda de los 3 clasificadores:\n",
    "moda=np.zeros(len(X_testSVC))\n",
    "ErrorModa=0\n",
    "for i in range(len(X_testSVC)):\n",
    "    if(salida1[i]+salida2[i]+salida3[i]>0):\n",
    "        moda[i]=1\n",
    "    else:\n",
    "        moda[i]=-1\n",
    "    if(moda[i]!=Y_test[i]):\n",
    "        ErrorModa=ErrorModa+1   \n",
    "        \n",
    "aciertoModa=(len(Y_test)-ErrorModa)*1.0/len(Y_test)\n",
    "\n",
    "#AREA BAJO LA CURVA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive1, true_positive1, thresholds1 = roc_curve(Y_test, salida1)\n",
    "auc1=auc(false_positive1,true_positive1)\n",
    "false_positive2, true_positive2, thresholds2 = roc_curve(Y_test, salida2)\n",
    "auc2=auc(false_positive2,true_positive2)\n",
    "false_positive3, true_positive3, thresholds3 = roc_curve(Y_test, salida3)\n",
    "auc3=auc(false_positive3,true_positive3)\n",
    "false_positivemoda, true_positivemoda, thresholdsmoda = roc_curve(Y_test, moda)\n",
    "aucmoda=auc(false_positivemoda,true_positivemoda)\n",
    "\n",
    "print \"\\nEl error del clasificador ExtraTrees es \" + str(1-acierto1) + \", con una region bajo la curva de \" + str(auc1)\n",
    "print \"\\nEl error del clasificador Bagging es \" + str(1-acierto2) + \", con una region bajo la curva de \" + str(auc2)\n",
    "print \"\\nEl error del clasificador SVC es \" + str(1-acierto3) + \", con una region bajo la curva de \" + str(auc3)\n",
    "print \"\\nEl erro del clasificador de la moda es \" + str(1-aciertoModa) + \", con una region bajo la curva de \" + str(aucmoda)\n",
    "\n",
    "\n",
    "#Se imprimen los topics de los 3 modelos:\n",
    "print(\"\\n\\n TOPICS PARA EL EXTRATREES\\n\\n\")\n",
    "print_top_words(ldaExtraTrees,tf_vectorizer.get_feature_names(),5)\n",
    "print(\"\\n\\n TOPICS PARA EL BAGGING\\n\\n\")\n",
    "print_top_words(ldaBagging,tf_vectorizer.get_feature_names(),5)\n",
    "print(\"\\n\\n TOPICS PARA EL SVC\\n\\n\")\n",
    "print_top_words(ldaSVC,tf_vectorizer.get_feature_names(),8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
